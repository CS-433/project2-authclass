{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419700b3-7581-42ae-80a6-4c11a8c8d8c2",
   "metadata": {},
   "source": [
    "# Sample Authors and Prepare Feeds for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1306dc-fecc-4b4b-9216-9019f77d51f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# May need to install packages seen in feature_extraction_functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfeature_extraction_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m team_seed \u001b[39m=\u001b[39m \u001b[39m13\u001b[39m \u001b[39m+\u001b[39m \u001b[39m4\u001b[39m \u001b[39m+\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      4\u001b[0m random\u001b[39m.\u001b[39mseed(team_seed) \n",
      "File \u001b[0;32m~/Documents/EPFL/Master/Machine Learning/project2-braindead/feature_extraction_functions.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# May need to install packages seen in feature_extraction_functions\n",
    "from feature_extraction_functions import *\n",
    "team_seed = 13 + 4 + 5\n",
    "random.seed(team_seed) \n",
    "\n",
    "# Space-split, long-format version of feeds (Arthur's version + some cleaning and wide-to-long conversion)\n",
    "eng_native = pd.read_pickle('Data/Classified/native_english_40feeds')\n",
    "eng_nonnat = pd.read_pickle('Data/Classified/non_native_english_40feeds')\n",
    "eng_native['proficiency'] = \"N\" # native\n",
    "eng_nonnat['proficiency'] = \"L\" # learner\n",
    "\n",
    "# Three cohorts will be created based on development split below.\n",
    "# Cohort 1: 15 Native + 15 Non-Native\n",
    "# Cohort 2: 15 Native from Cohort 1 + another random 15 native\n",
    "# Cohort 3: 15 Non-Native from Cohort 2 + another random 15 non-native\n",
    "# Therefore, we need 30 native and 30 non-native authors for the development cohorts\n",
    "num_native_authors_to_sample = 30\n",
    "num_nonnat_authors_to_sample = 30\n",
    "eng_native_sample = eng_native.sample(num_native_authors_to_sample, random_state=team_seed)\n",
    "eng_nonnat_sample = eng_nonnat.sample(num_nonnat_authors_to_sample, random_state=team_seed)\n",
    "\n",
    "### ARTHUR REMOVE THIS ONCE YOU LIMIT TO 20 FEEDS PER AUTHOR ###\n",
    "eng_native_sample = eng_native_sample[['timerange', 'file1', 'slices1', 'file2', 'slices2', 'file3', 'slices3',\n",
    "       'file4', 'slices4', 'file5', 'slices5', 'file6', 'slices6', 'file7',\n",
    "       'slices7', 'file8', 'slices8', 'file9', 'slices9', 'file10', 'slices10',\n",
    "       'file11', 'slices11', 'file12', 'slices12', 'file13', 'slices13',\n",
    "       'file14', 'slices14', 'file15', 'slices15', 'file16', 'slices16',\n",
    "       'file17', 'slices17', 'file18', 'slices18', 'file19', 'slices19',\n",
    "       'file20', 'slices20', 'proficiency']]\n",
    "\n",
    "eng_nonnat_sample = eng_nonnat_sample[['timerange', 'file1', 'slices1', 'file2', 'slices2', 'file3', 'slices3',\n",
    "       'file4', 'slices4', 'file5', 'slices5', 'file6', 'slices6', 'file7',\n",
    "       'slices7', 'file8', 'slices8', 'file9', 'slices9', 'file10', 'slices10',\n",
    "       'file11', 'slices11', 'file12', 'slices12', 'file13', 'slices13',\n",
    "       'file14', 'slices14', 'file15', 'slices15', 'file16', 'slices16',\n",
    "       'file17', 'slices17', 'file18', 'slices18', 'file19', 'slices19',\n",
    "       'file20', 'slices20', 'proficiency']]\n",
    "\n",
    "# Before splitting into cohorts, perform all pre-feature-extraction processing\n",
    "eng_feeds = pd.concat([eng_nonnat_sample, eng_native_sample], ignore_index=False, axis=0) # \n",
    "eng_feeds['author'] = eng_feeds.index\n",
    "eng_feeds = pd.wide_to_long(eng_feeds, [\"file\", \"slices\"], i=\"author\", j=\"intra_author_feed_id\").sort_index()\n",
    "eng_feeds = eng_feeds.rename(columns={\"slices\": \"comment_lengths\", \"file\": \"feed_tokens_space\"})\n",
    "\n",
    "# Raw string version of feeds\n",
    "def feed_string(feed):\n",
    "    return ' '.join(feed)\n",
    "eng_feeds['feed_string'] = eng_feeds['feed_tokens_space'].apply(feed_string)\n",
    "\n",
    "# List-of-comments version of feeds\n",
    "## Doing this via loop because two columns involved in function instead of one...\n",
    "def create_comment_word_indices(comment_lengths):\n",
    "    np_comment_lengths = np.array(comment_lengths)\n",
    "    return np.cumsum(np_comment_lengths)\n",
    "eng_feeds['comment_word_indices'] = eng_feeds['comment_lengths'].apply(create_comment_word_indices)\n",
    "\n",
    "eng_feeds['feed_comment_list'] = \"\"\n",
    "for index, row in eng_feeds.iterrows():\n",
    "    comm_w_indices_temp = row['comment_word_indices']\n",
    "    feed_tokens_space_temp = row['feed_tokens_space']\n",
    "    inner_list = []\n",
    "    for i in range(len(comm_w_indices_temp)):\n",
    "        if i == 0:\n",
    "            inner_list.append(feed_tokens_space_temp[0:comm_w_indices_temp[i]])\n",
    "        else:\n",
    "            inner_list.append(feed_tokens_space_temp[comm_w_indices_temp[i-1]:comm_w_indices_temp[i]])\n",
    "    eng_feeds.at[index,'feed_comment_list'] = inner_list\n",
    "eng_feeds = eng_feeds.drop('comment_word_indices', axis=1)\n",
    "\n",
    "# List-of-comments w/ punctuation stripped and lowercase applied version of feeds\n",
    "def strip_punc_and_lower_nested_list(feed_comment_list):\n",
    "    feed_comment_list_nopunc_lower = []\n",
    "    for comment in feed_comment_list:\n",
    "        feed_comment_list_nopunc_lower.append(re.sub(r'[^A-Za-z0-9 ]+', '', ' '.join(comment)).lower().split()) \n",
    "    return feed_comment_list_nopunc_lower\n",
    "eng_feeds['feed_comment_list_nopunc_lower'] = eng_feeds['feed_comment_list'].apply(strip_punc_and_lower_nested_list)\n",
    "\n",
    "# List-of-comments Spacy-tokenized version of feeds\n",
    "tokenizer_wrapper(eng_feeds, 'feed_comment_list')\n",
    "\n",
    "eng_feeds.to_pickle(\"eng_development_feeds_pre_split.pkl\")\n",
    "\n",
    "eng_feeds.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cbbeec-dd40-4f85-8c91-e761724ba311",
   "metadata": {},
   "source": [
    "# Encode author and proficiency levels, Split into Three Development Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069a814-12fb-4c61-8054-f73cd2c00579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2022-12-08 13:05:52.707667: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-08 13:05:59.534712: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-08 13:05:59.534753: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-08 13:06:20.841730: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-08 13:06:20.843546: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-08 13:06:20.843579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-08 13:06:47.477131: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-08 13:06:47.518567: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-08 13:06:47.518711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (noto.epfl.ch): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from feature_extraction_functions import *\n",
    "from random import sample\n",
    "team_seed = 13 + 4 + 5\n",
    "random.seed(team_seed) \n",
    "\n",
    "eng_feeds = pd.read_pickle(\"eng_development_feeds_pre_split.pkl\")\n",
    "\n",
    "# Encode author and proficiency as numbers\n",
    "labelencoder = LabelEncoder()\n",
    "eng_feeds = eng_feeds.reset_index()\n",
    "t = eng_feeds['author']\n",
    "t = labelencoder.fit_transform(t)\n",
    "eng_feeds['author_id'] = t.tolist()\n",
    "t = eng_feeds['proficiency']\n",
    "t = labelencoder.fit_transform(t)\n",
    "eng_feeds['proficiency_id'] = t.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce6f7a-9e15-4ab8-ac2a-10d53ef93a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 60 authors into three cohorts\n",
    "# Cohort 1: 15 native + 15 non-native\n",
    "native_authors = list(set(eng_feeds[eng_feeds['proficiency'] == \"N\"]['author_id'].values))\n",
    "nonnat_authors = list(set(eng_feeds[eng_feeds['proficiency'] == \"L\"]['author_id'].values))\n",
    "all_cohort_native_subset = sample(native_authors, int(num_native_authors_to_sample / 2))\n",
    "all_cohort_nonnat_subset = sample(nonnat_authors, int(num_nonnat_authors_to_sample / 2))\n",
    "cohort_all = pd.concat([eng_feeds[eng_feeds['author_id'].isin(all_cohort_native_subset)], eng_feeds[eng_feeds['author_id'].isin(all_cohort_nonnat_subset)]], ignore_index=False, axis=0)\n",
    "# Cohort 2: 30 native\n",
    "cohort_native = eng_feeds[eng_feeds['proficiency'] == \"N\"]\n",
    "# Cohort 3: 30 non-native\n",
    "cohort_nonnat = eng_feeds[eng_feeds['proficiency'] == \"L\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d07203-7b2d-408d-8119-d8d77af63a33",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b62e6-6eae-48a2-8ef4-650e3a8b3933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing letter count & proportion...\n",
      "Performed letter count & proportion in 0.9313466548919678 seconds\n",
      "Performing digit count & proportion...\n",
      "Performed digit count & proportion in 0.7103791236877441 seconds\n",
      "Performing punctuation count & proportion...\n",
      "Performed punctuation count & proportion in 1.6114833354949951 seconds\n",
      "Performing whitespace count & proportion...\n",
      "Performed whitespace count & proportion in 0.6121082305908203 seconds\n",
      "Performing character count & proportion...\n",
      "Performed character count & proportion in 0.03362536430358887 seconds\n",
      "Performing word count...\n",
      "Performed word count in 0.1242973804473877 seconds\n",
      "Performing word length avg...\n",
      "Performed word length avg in 0.1710972785949707 seconds\n",
      "Performing word length distribution...\n",
      "Performed word length distribution in 0.27648448944091797 seconds\n",
      "Performing word count short...\n",
      "Performed word count short in 0.0027611255645751953 seconds\n",
      "Performing letter case distribution...\n",
      "Performed letter case distribution in 0.3502538204193115 seconds\n",
      "Performing word case distribution...\n",
      "Performed word case distribution in 0.5647554397583008 seconds\n",
      "Performing misspellings proportion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bde05d80efe4b05a280c12a0cb33d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed misspellings proportion in 1195.3415365219116 seconds\n",
      "Performing stop words ratio...\n",
      "Performed stop words ratio in 1.826256275177002 seconds\n",
      "Performing hapax legomena proportion of total tokens...\n",
      "Performed hapax legomena proportion of total tokens in 1.915389060974121 seconds\n",
      "Performing hapax legomena proportion of unique tokens...\n",
      "Performed hapax legomena proportion of unique tokens in 1.815678358078003 seconds\n",
      "Performing token type ratio...\n",
      "Performed token type ratio in 2.9435532093048096 seconds\n",
      "Performing train letter 1-gram...\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0bd3b0fcce4b9f9cdd8d03cb86d747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 1-gram in 3.4877285957336426 seconds\n",
      "Returned up to 50 most common letter 1-grams for feature extraction on test set.\n",
      "Performing train letter 2-gram...\n",
      "['al', 'an', 'ar', 'as', 'at', 'be', 'ca', 'co', 'de', 'ea', 'ed', 'el', 'en', 'er', 'es', 'ha', 'he', 'hi', 'ho', 'in', 'is', 'it', 'le', 'li', 'll', 'ly', 'me', 'nd', 'ne', 'ng', 'no', 'nt', 'of', 'om', 'on', 'or', 'ot', 'ou', 're', 'se', 'so', 'st', 'te', 'th', 'ti', 'to', 'us', 'ut', 've', 'yo']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4433672ec73a45f1aa8f5fdc9e4cd7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 2-gram in 10.574246406555176 seconds\n",
      "Returned up to 50 most common letter 2-grams for feature extraction on test set.\n",
      "Performing train letter 3-gram...\n",
      "['all', 'and', 'are', 'ate', 'ati', 'ave', 'but', 'can', 'com', 'con', 'ear', 'ent', 'ere', 'ers', 'eve', 'for', 'hat', 'hav', 'her', 'hey', 'hin', 'his', 'ike', 'ing', 'ion', 'ith', 'ive', 'lly', 'nce', 'not', 'ome', 'one', 'oul', 'our', 'out', 'ple', 'rea', 'som', 'ter', 'tha', 'the', 'thi', 'tin', 'tio', 'uld', 'use', 'ust', 'ver', 'wit', 'you']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d23c764746a48308867d5407e93a409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 3-gram in 177.62541508674622 seconds\n",
      "Returned up to 50 most common letter 3-grams for feature extraction on test set.\n",
      "Performing train digit 1-gram...\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2780986e8da3414d82449e6d37c8b494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 1-gram in 2.6545698642730713 seconds\n",
      "Returned up to 50 most common digit 1-grams for feature extraction on test set.\n",
      "Performing train digit 2-gram...\n",
      "['00', '01', '02', '03', '04', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '30', '31', '32', '33', '35', '36', '40', '41', '45', '49', '50', '52', '55', '60', '61', '65', '66', '68', '70', '71', '79', '80', '88', '90', '95', '96', '99']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25babb5d97d49c88e2bbdaa9bc21905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 2-gram in 4.833482980728149 seconds\n",
      "Returned up to 50 most common digit 2-grams for feature extraction on test set.\n",
      "Performing train digit 3-gram...\n",
      "['000', '001', '008', '013', '014', '016', '018', '019', '020', '021', '022', '045', '090', '100', '112', '120', '121', '122', '124', '125', '130', '150', '156', '166', '168', '180', '195', '199', '200', '201', '202', '212', '220', '241', '250', '255', '300', '330', '400', '415', '500', '600', '613', '647', '680', '700', '716', '800', '804', '900']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feb655abeb84c2c91aeadf23e9486d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 3-gram in 20.73089075088501 seconds\n",
      "Returned up to 50 most common digit 3-grams for feature extraction on test set.\n",
      "Performing train punctuation 1-gram...\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '}', '~', '’', '“', '”']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c313012f0a46c0a3353d4377671792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train punctuation 1-gram in 3.4987194538116455 seconds\n",
      "Returned up to 50 most common punctuation 1-grams for feature extraction on test set.\n",
      "Performing train punctuation 2-gram...\n",
      "['!!', '!\"', '!)', '\")', '\",', '\".', '\"?', '%.', '&#', \"',\", \"'.\", '(*', '(+', '(/', '))', '),', ').', ');', '*)', '**', '*,', '*.', ',\"', '-)', '--', '.\"', '.)', '.*', '.,', '..', '.]', '.”', '/)', '//', '/?', '/]', ':(', ':)', ':-', ':/', '?!', '?\"', '?)', '??', '\\\\*', '\\\\-', '\\\\[', '\\\\]', '\\\\_', '](']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58782f27f14e4d9ebb6995f5cc54ccea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train punctuation 2-gram in 32.301445722579956 seconds\n",
      "Returned up to 50 most common punctuation 2-grams for feature extraction on test set.\n",
      "Performing train word 1-gram...\n",
      "[('a',), ('about',), ('all',), ('an',), ('and',), ('are',), ('as',), ('at',), ('be',), ('but',), ('can',), ('do',), ('dont',), ('for',), ('from',), ('get',), ('have',), ('i',), ('if',), ('im',), ('in',), ('is',), ('it',), ('its',), ('just',), ('like',), ('me',), ('more',), ('my',), ('not',), ('of',), ('on',), ('one',), ('or',), ('people',), ('so',), ('that',), ('the',), ('there',), ('they',), ('think',), ('this',), ('to',), ('was',), ('what',), ('when',), ('with',), ('would',), ('you',), ('your',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed82efed31a474e8abfcfd86da14b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train word 1-gram in 69.33535146713257 seconds\n",
      "Returned up to 50 most common word1-grams for feature extraction on test set.\n",
      "Performing train POS tags 1-grams...\n",
      "[('$',), (\"''\",), (',',), ('-LRB-',), ('-RRB-',), ('.',), (':',), ('ADD',), ('AFX',), ('CC',), ('CD',), ('DT',), ('EX',), ('FW',), ('HYPH',), ('IN',), ('JJ',), ('JJR',), ('JJS',), ('LS',), ('MD',), ('NFP',), ('NN',), ('NNP',), ('NNPS',), ('NNS',), ('PDT',), ('POS',), ('PRP',), ('PRP$',), ('RB',), ('RBR',), ('RBS',), ('RP',), ('SYM',), ('TO',), ('UH',), ('VB',), ('VBD',), ('VBG',), ('VBN',), ('VBP',), ('VBZ',), ('WDT',), ('WP',), ('WP$',), ('WRB',), ('XX',), ('_SP',), ('``',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d25fb40950446948c617019714ad692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train POS tags 1-gram in 1.348522424697876 seconds\n",
      "Returned up to 50 most common POS tags 1-grams for feature extraction on test set.\n",
      "Performing train POS tags 2-grams...\n",
      "[(',', 'CC'), (',', 'PRP'), ('.', 'PRP'), ('CC', 'PRP'), ('DT', 'JJ'), ('DT', 'NN'), ('DT', 'NNS'), ('IN', 'DT'), ('IN', 'JJ'), ('IN', 'NN'), ('IN', 'NNP'), ('IN', 'NNS'), ('IN', 'PRP'), ('IN', 'PRP$'), ('IN', 'VBG'), ('JJ', '.'), ('JJ', 'IN'), ('JJ', 'NN'), ('JJ', 'NNS'), ('MD', 'RB'), ('MD', 'VB'), ('NN', ','), ('NN', '.'), ('NN', 'CC'), ('NN', 'IN'), ('NN', 'NN'), ('NN', 'RB'), ('NN', 'VBZ'), ('NNP', 'NNP'), ('NNS', '.'), ('NNS', 'IN'), ('PRP', 'MD'), ('PRP', 'RB'), ('PRP', 'VBD'), ('PRP', 'VBP'), ('PRP', 'VBZ'), ('PRP$', 'NN'), ('RB', '.'), ('RB', 'IN'), ('RB', 'JJ'), ('RB', 'RB'), ('RB', 'VB'), ('TO', 'VB'), ('VB', 'DT'), ('VB', 'IN'), ('VB', 'PRP'), ('VBN', 'IN'), ('VBP', 'RB'), ('VBZ', 'DT'), ('VBZ', 'RB')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd59606849624a878097c32650faf367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train POS tags 2-gram in 9.31908917427063 seconds\n",
      "Returned up to 50 most common POS tags 2-grams for feature extraction on test set.\n",
      "Performing letter count & proportion...\n",
      "Performed letter count & proportion in 0.11255812644958496 seconds\n",
      "Performing digit count & proportion...\n",
      "Performed digit count & proportion in 0.08694267272949219 seconds\n",
      "Performing punctuation count & proportion...\n",
      "Performed punctuation count & proportion in 0.16901087760925293 seconds\n",
      "Performing whitespace count & proportion...\n",
      "Performed whitespace count & proportion in 0.06548333168029785 seconds\n",
      "Performing character count & proportion...\n",
      "Performed character count & proportion in 0.008797883987426758 seconds\n",
      "Performing word count...\n",
      "Performed word count in 0.011553764343261719 seconds\n",
      "Performing word length avg...\n",
      "Performed word length avg in 0.018317461013793945 seconds\n",
      "Performing word length distribution...\n",
      "Performed word length distribution in 0.029773712158203125 seconds\n",
      "Performing word count short...\n",
      "Performed word count short in 0.003164052963256836 seconds\n",
      "Performing letter case distribution...\n",
      "Performed letter case distribution in 0.04065275192260742 seconds\n",
      "Performing word case distribution...\n",
      "Performed word case distribution in 0.06730294227600098 seconds\n",
      "Performing misspellings proportion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed55272e25644a6aa87c2287c231cfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed misspellings proportion in 130.78419303894043 seconds\n",
      "Performing stop words ratio...\n",
      "Performed stop words ratio in 0.21990680694580078 seconds\n",
      "Performing hapax legomena proportion of total tokens...\n",
      "Performed hapax legomena proportion of total tokens in 0.18541932106018066 seconds\n",
      "Performing hapax legomena proportion of unique tokens...\n",
      "Performed hapax legomena proportion of unique tokens in 0.22136473655700684 seconds\n",
      "Performing token type ratio...\n",
      "Performed token type ratio in 1.2757995128631592 seconds\n",
      "Performing test letter 1-gram...\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9fe6caa7914444b980865f68313088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 1-gram in 0.19907712936401367 seconds\n",
      "Performing test letter 2-gram...\n",
      "['al', 'an', 'ar', 'as', 'at', 'be', 'ca', 'co', 'de', 'ea', 'ed', 'el', 'en', 'er', 'es', 'ha', 'he', 'hi', 'ho', 'in', 'is', 'it', 'le', 'li', 'll', 'ly', 'me', 'nd', 'ne', 'ng', 'no', 'nt', 'of', 'om', 'on', 'or', 'ot', 'ou', 're', 'se', 'so', 'st', 'te', 'th', 'ti', 'to', 'us', 'ut', 've', 'yo']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab5ef43222649ca8575e48adda29144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 2-gram in 0.2587268352508545 seconds\n",
      "Performing test letter 3-gram...\n",
      "['all', 'and', 'are', 'ate', 'ati', 'ave', 'but', 'can', 'com', 'con', 'ear', 'ent', 'ere', 'ers', 'eve', 'for', 'hat', 'hav', 'her', 'hey', 'hin', 'his', 'ike', 'ing', 'ion', 'ith', 'ive', 'lly', 'nce', 'not', 'ome', 'one', 'oul', 'our', 'out', 'ple', 'rea', 'som', 'ter', 'tha', 'the', 'thi', 'tin', 'tio', 'uld', 'use', 'ust', 'ver', 'wit', 'you']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6806d100546943d98d2a11962600d1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 3-gram in 0.22792768478393555 seconds\n",
      "Performing test digit 1-gram...\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ea16c8d5d34a3b96167be4eb1408b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 1-gram in 0.26302099227905273 seconds\n",
      "Performing test digit 2-gram...\n",
      "['00', '01', '02', '03', '04', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '30', '31', '32', '33', '35', '36', '40', '41', '45', '49', '50', '52', '55', '60', '61', '65', '66', '68', '70', '71', '79', '80', '88', '90', '95', '96', '99']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af73a17bde0d440c8aa783b57bea5fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 2-gram in 0.22809767723083496 seconds\n",
      "Performing test digit 3-gram...\n",
      "['000', '001', '008', '013', '014', '016', '018', '019', '020', '021', '022', '045', '090', '100', '112', '120', '121', '122', '124', '125', '130', '150', '156', '166', '168', '180', '195', '199', '200', '201', '202', '212', '220', '241', '250', '255', '300', '330', '400', '415', '500', '600', '613', '647', '680', '700', '716', '800', '804', '900']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc5af9dc1a04f979357266c0868dc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 3-gram in 0.20425820350646973 seconds\n",
      "Performing test punctuation 1-gram...\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '}', '~', '’', '“', '”']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1263ae8191114d5c87761cf3ff81e5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test punctuation 1-gram in 0.2701423168182373 seconds\n",
      "Performing test punctuation 2-gram...\n",
      "['!!', '!\"', '!)', '\")', '\",', '\".', '\"?', '%.', '&#', \"',\", \"'.\", '(*', '(+', '(/', '))', '),', ').', ');', '*)', '**', '*,', '*.', ',\"', '-)', '--', '.\"', '.)', '.*', '.,', '..', '.]', '.”', '/)', '//', '/?', '/]', ':(', ':)', ':-', ':/', '?!', '?\"', '?)', '??', '\\\\*', '\\\\-', '\\\\[', '\\\\]', '\\\\_', '](']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d512f34c5cc04d85856c972449be7abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test punctuation 2-gram in 0.22701215744018555 seconds\n",
      "Performing test word 1-gram...\n",
      "[('a',), ('about',), ('all',), ('an',), ('and',), ('are',), ('as',), ('at',), ('be',), ('but',), ('can',), ('do',), ('dont',), ('for',), ('from',), ('get',), ('have',), ('i',), ('if',), ('im',), ('in',), ('is',), ('it',), ('its',), ('just',), ('like',), ('me',), ('more',), ('my',), ('not',), ('of',), ('on',), ('one',), ('or',), ('people',), ('so',), ('that',), ('the',), ('there',), ('they',), ('think',), ('this',), ('to',), ('was',), ('what',), ('when',), ('with',), ('would',), ('you',), ('your',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5595e8a815a640e4982c8eb056ac2c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test word 1-gram in 0.27701401710510254 seconds\n",
      "Performing test POS tags 1-gram...\n",
      "[('$',), (\"''\",), (',',), ('-LRB-',), ('-RRB-',), ('.',), (':',), ('ADD',), ('AFX',), ('CC',), ('CD',), ('DT',), ('EX',), ('FW',), ('HYPH',), ('IN',), ('JJ',), ('JJR',), ('JJS',), ('LS',), ('MD',), ('NFP',), ('NN',), ('NNP',), ('NNPS',), ('NNS',), ('PDT',), ('POS',), ('PRP',), ('PRP$',), ('RB',), ('RBR',), ('RBS',), ('RP',), ('SYM',), ('TO',), ('UH',), ('VB',), ('VBD',), ('VBG',), ('VBN',), ('VBP',), ('VBZ',), ('WDT',), ('WP',), ('WP$',), ('WRB',), ('XX',), ('_SP',), ('``',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f284ce7b8b742b1856a93e22da9525a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test POS tags 1-gram in 0.18973064422607422 seconds\n",
      "Performing test POS tags 2-gram...\n",
      "[(',', 'CC'), (',', 'PRP'), ('.', 'PRP'), ('CC', 'PRP'), ('DT', 'JJ'), ('DT', 'NN'), ('DT', 'NNS'), ('IN', 'DT'), ('IN', 'JJ'), ('IN', 'NN'), ('IN', 'NNP'), ('IN', 'NNS'), ('IN', 'PRP'), ('IN', 'PRP$'), ('IN', 'VBG'), ('JJ', '.'), ('JJ', 'IN'), ('JJ', 'NN'), ('JJ', 'NNS'), ('MD', 'RB'), ('MD', 'VB'), ('NN', ','), ('NN', '.'), ('NN', 'CC'), ('NN', 'IN'), ('NN', 'NN'), ('NN', 'RB'), ('NN', 'VBZ'), ('NNP', 'NNP'), ('NNS', '.'), ('NNS', 'IN'), ('PRP', 'MD'), ('PRP', 'RB'), ('PRP', 'VBD'), ('PRP', 'VBP'), ('PRP', 'VBZ'), ('PRP$', 'NN'), ('RB', '.'), ('RB', 'IN'), ('RB', 'JJ'), ('RB', 'RB'), ('RB', 'VB'), ('TO', 'VB'), ('VB', 'DT'), ('VB', 'IN'), ('VB', 'PRP'), ('VBN', 'IN'), ('VBP', 'RB'), ('VBZ', 'DT'), ('VBZ', 'RB')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747d35e630904cabb403e39144b375a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test POS tags 2-gram in 0.3223552703857422 seconds\n",
      "Performing letter count & proportion...\n",
      "Performed letter count & proportion in 0.8379025459289551 seconds\n",
      "Performing digit count & proportion...\n",
      "Performed digit count & proportion in 0.7659454345703125 seconds\n",
      "Performing punctuation count & proportion...\n",
      "Performed punctuation count & proportion in 1.5017914772033691 seconds\n",
      "Performing whitespace count & proportion...\n",
      "Performed whitespace count & proportion in 0.5957372188568115 seconds\n",
      "Performing character count & proportion...\n",
      "Performed character count & proportion in 0.023203372955322266 seconds\n",
      "Performing word count...\n",
      "Performed word count in 0.10275959968566895 seconds\n",
      "Performing word length avg...\n",
      "Performed word length avg in 0.12980198860168457 seconds\n",
      "Performing word length distribution...\n",
      "Performed word length distribution in 0.26637887954711914 seconds\n",
      "Performing word count short...\n",
      "Performed word count short in 0.0039446353912353516 seconds\n",
      "Performing letter case distribution...\n",
      "Performed letter case distribution in 0.33636999130249023 seconds\n",
      "Performing word case distribution...\n",
      "Performed word case distribution in 0.5675544738769531 seconds\n",
      "Performing misspellings proportion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280adb6690c34a39b79b4bb54f230d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed misspellings proportion in 1100.2819290161133 seconds\n",
      "Performing stop words ratio...\n",
      "Performed stop words ratio in 1.966287612915039 seconds\n",
      "Performing hapax legomena proportion of total tokens...\n",
      "Performed hapax legomena proportion of total tokens in 1.745077133178711 seconds\n",
      "Performing hapax legomena proportion of unique tokens...\n",
      "Performed hapax legomena proportion of unique tokens in 1.9414589405059814 seconds\n",
      "Performing token type ratio...\n",
      "Performed token type ratio in 2.955264091491699 seconds\n",
      "Performing train letter 1-gram...\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e86e13f2b74bf8bddffcfecd2161e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 1-gram in 3.518671989440918 seconds\n",
      "Returned up to 50 most common letter 1-grams for feature extraction on test set.\n",
      "Performing train letter 2-gram...\n",
      "['al', 'an', 'ar', 'as', 'at', 'be', 'ca', 'co', 'de', 'ea', 'ed', 'en', 'er', 'es', 'ha', 'he', 'hi', 'ho', 'in', 'is', 'it', 'le', 'li', 'll', 'ly', 'me', 'nd', 'ne', 'ng', 'no', 'nt', 'of', 'om', 'on', 'or', 'ot', 'ou', 're', 'ri', 'se', 'so', 'st', 'te', 'th', 'ti', 'to', 'us', 'ut', 've', 'yo']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7947c0527dad4c1eae290fe82b77b2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 2-gram in 10.98355507850647 seconds\n",
      "Returned up to 50 most common letter 2-grams for feature extraction on test set.\n",
      "Performing train letter 3-gram...\n",
      "['all', 'and', 'are', 'ate', 'ati', 'ave', 'but', 'can', 'com', 'con', 'ear', 'ent', 'ere', 'ers', 'eve', 'for', 'hat', 'hav', 'her', 'hey', 'hin', 'his', 'ike', 'ing', 'ion', 'ith', 'ive', 'lik', 'lly', 'not', 'ome', 'one', 'ore', 'oul', 'our', 'out', 'ple', 'rea', 'som', 'ter', 'tha', 'the', 'thi', 'tin', 'tio', 'uld', 'use', 'ver', 'wit', 'you']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e24276169c84c6d91399056699890f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 3-gram in 182.03345775604248 seconds\n",
      "Returned up to 50 most common letter 3-grams for feature extraction on test set.\n",
      "Performing train digit 1-gram...\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808662f452fc446ba87f943170c08004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 1-gram in 2.52227520942688 seconds\n",
      "Returned up to 50 most common digit 1-grams for feature extraction on test set.\n",
      "Performing train digit 2-gram...\n",
      "['00', '01', '02', '03', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '30', '31', '32', '33', '40', '42', '45', '49', '50', '52', '60', '65', '70', '72', '75', '79', '80', '90', '95', '96', '97', '98', '99']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a083f2017fbb47f390ba3e83b8bc45df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 2-gram in 4.991602897644043 seconds\n",
      "Returned up to 50 most common digit 2-grams for feature extraction on test set.\n",
      "Performing train digit 3-gram...\n",
      "['000', '001', '006', '010', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '045', '080', '090', '100', '112', '120', '125', '147', '149', '150', '168', '180', '190', '195', '197', '198', '199', '200', '201', '202', '250', '300', '320', '400', '471', '500', '512', '600', '680', '700', '716', '731', '800', '804', '900', '979']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0555a8cedf9c40c193adb650f299fb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 3-gram in 20.148253202438354 seconds\n",
      "Returned up to 50 most common digit 3-grams for feature extraction on test set.\n",
      "Performing train punctuation 1-gram...\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '’', '“', '”']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d336a7e44840f48e6cb2288eaf6626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train punctuation 1-gram in 4.43937349319458 seconds\n",
      "Returned up to 50 most common punctuation 1-grams for feature extraction on test set.\n",
      "Performing train punctuation 2-gram...\n",
      "['!!', '!\"', '!)', '!<', '!?', '!”', '\")', '\",', '\".', '\"?', '%.', \"'.\", '(*', '(/', '))', '),', ').', '*)', '**', '*,', '*.', ',\"', ',”', '-)', '--', '->', '.\"', '.)', '.*', '.,', '..', '.]', '.”', '/)', '//', '/?', '/]', ':)', ':-', ':/', '?!', '?\"', '?)', '??', '?”', '\\\\*', '\\\\-', '\\\\_', '](', '~~']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13de82815d11417993c9a99d51700114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train punctuation 2-gram in 32.49608492851257 seconds\n",
      "Returned up to 50 most common punctuation 2-grams for feature extraction on test set.\n",
      "Performing train word 1-gram...\n",
      "[('a',), ('about',), ('all',), ('an',), ('and',), ('are',), ('as',), ('at',), ('be',), ('but',), ('can',), ('do',), ('dont',), ('for',), ('from',), ('get',), ('have',), ('i',), ('if',), ('im',), ('in',), ('is',), ('it',), ('its',), ('just',), ('like',), ('me',), ('more',), ('my',), ('not',), ('of',), ('on',), ('one',), ('or',), ('out',), ('people',), ('so',), ('some',), ('that',), ('the',), ('they',), ('this',), ('to',), ('was',), ('we',), ('what',), ('with',), ('would',), ('you',), ('your',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c140d50cb048414b85cdd09274c63948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train word 1-gram in 64.47618269920349 seconds\n",
      "Returned up to 50 most common word1-grams for feature extraction on test set.\n",
      "Performing train POS tags 1-grams...\n",
      "[('$',), (\"''\",), (',',), ('-LRB-',), ('-RRB-',), ('.',), (':',), ('ADD',), ('AFX',), ('CC',), ('CD',), ('DT',), ('EX',), ('FW',), ('HYPH',), ('IN',), ('JJ',), ('JJR',), ('JJS',), ('LS',), ('MD',), ('NFP',), ('NN',), ('NNP',), ('NNPS',), ('NNS',), ('PDT',), ('POS',), ('PRP',), ('PRP$',), ('RB',), ('RBR',), ('RBS',), ('RP',), ('SYM',), ('TO',), ('UH',), ('VB',), ('VBD',), ('VBG',), ('VBN',), ('VBP',), ('VBZ',), ('WDT',), ('WP',), ('WP$',), ('WRB',), ('XX',), ('_SP',), ('``',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b77f9692e834430add02c3ae84468fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train POS tags 1-gram in 1.3292150497436523 seconds\n",
      "Returned up to 50 most common POS tags 1-grams for feature extraction on test set.\n",
      "Performing train POS tags 2-grams...\n",
      "[(',', 'CC'), (',', 'PRP'), ('.', 'PRP'), ('.', 'RB'), ('CC', 'PRP'), ('DT', 'JJ'), ('DT', 'NN'), ('DT', 'NNS'), ('IN', 'DT'), ('IN', 'JJ'), ('IN', 'NN'), ('IN', 'NNP'), ('IN', 'NNS'), ('IN', 'PRP'), ('IN', 'PRP$'), ('JJ', '.'), ('JJ', 'IN'), ('JJ', 'NN'), ('JJ', 'NNS'), ('MD', 'VB'), ('NN', ','), ('NN', '.'), ('NN', 'CC'), ('NN', 'IN'), ('NN', 'NN'), ('NN', 'NNS'), ('NN', 'VBZ'), ('NNP', 'NNP'), ('NNS', '.'), ('NNS', 'IN'), ('PRP', 'MD'), ('PRP', 'RB'), ('PRP', 'VBD'), ('PRP', 'VBP'), ('PRP', 'VBZ'), ('PRP$', 'NN'), ('RB', '.'), ('RB', 'IN'), ('RB', 'JJ'), ('RB', 'RB'), ('RB', 'VB'), ('TO', 'VB'), ('VB', 'DT'), ('VB', 'IN'), ('VB', 'PRP'), ('VBN', 'IN'), ('VBP', 'DT'), ('VBP', 'RB'), ('VBZ', 'DT'), ('VBZ', 'RB')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa2fb328fd74aef904d4d90a990d5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train POS tags 2-gram in 8.291099071502686 seconds\n",
      "Returned up to 50 most common POS tags 2-grams for feature extraction on test set.\n",
      "Performing letter count & proportion...\n",
      "Performed letter count & proportion in 0.10777640342712402 seconds\n",
      "Performing digit count & proportion...\n",
      "Performed digit count & proportion in 0.07855224609375 seconds\n",
      "Performing punctuation count & proportion...\n",
      "Performed punctuation count & proportion in 0.19692683219909668 seconds\n",
      "Performing whitespace count & proportion...\n",
      "Performed whitespace count & proportion in 0.05801582336425781 seconds\n",
      "Performing character count & proportion...\n",
      "Performed character count & proportion in 0.006568431854248047 seconds\n",
      "Performing word count...\n",
      "Performed word count in 0.012842416763305664 seconds\n",
      "Performing word length avg...\n",
      "Performed word length avg in 0.015816688537597656 seconds\n",
      "Performing word length distribution...\n",
      "Performed word length distribution in 0.024096965789794922 seconds\n",
      "Performing word count short...\n",
      "Performed word count short in 0.0018618106842041016 seconds\n",
      "Performing letter case distribution...\n",
      "Performed letter case distribution in 0.040753841400146484 seconds\n",
      "Performing word case distribution...\n",
      "Performed word case distribution in 0.05630230903625488 seconds\n",
      "Performing misspellings proportion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd8b6efb5984779b81d2fa609402896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed misspellings proportion in 110.7400336265564 seconds\n",
      "Performing stop words ratio...\n",
      "Performed stop words ratio in 0.23448944091796875 seconds\n",
      "Performing hapax legomena proportion of total tokens...\n",
      "Performed hapax legomena proportion of total tokens in 0.24635744094848633 seconds\n",
      "Performing hapax legomena proportion of unique tokens...\n",
      "Performed hapax legomena proportion of unique tokens in 0.22994637489318848 seconds\n",
      "Performing token type ratio...\n",
      "Performed token type ratio in 1.3524479866027832 seconds\n",
      "Performing test letter 1-gram...\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d7bef941aa4cdaad3fa01b76353e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 1-gram in 0.23977947235107422 seconds\n",
      "Performing test letter 2-gram...\n",
      "['al', 'an', 'ar', 'as', 'at', 'be', 'ca', 'co', 'de', 'ea', 'ed', 'en', 'er', 'es', 'ha', 'he', 'hi', 'ho', 'in', 'is', 'it', 'le', 'li', 'll', 'ly', 'me', 'nd', 'ne', 'ng', 'no', 'nt', 'of', 'om', 'on', 'or', 'ot', 'ou', 're', 'ri', 'se', 'so', 'st', 'te', 'th', 'ti', 'to', 'us', 'ut', 've', 'yo']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0df27d0a95e40a1b895e36cb537eaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 2-gram in 0.24193811416625977 seconds\n",
      "Performing test letter 3-gram...\n",
      "['all', 'and', 'are', 'ate', 'ati', 'ave', 'but', 'can', 'com', 'con', 'ear', 'ent', 'ere', 'ers', 'eve', 'for', 'hat', 'hav', 'her', 'hey', 'hin', 'his', 'ike', 'ing', 'ion', 'ith', 'ive', 'lik', 'lly', 'not', 'ome', 'one', 'ore', 'oul', 'our', 'out', 'ple', 'rea', 'som', 'ter', 'tha', 'the', 'thi', 'tin', 'tio', 'uld', 'use', 'ver', 'wit', 'you']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb508d1edefc4935af7f04bef12a3063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 3-gram in 0.1956470012664795 seconds\n",
      "Performing test digit 1-gram...\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e89b16a5b144ceb94e0a3437f3ece4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 1-gram in 0.26711606979370117 seconds\n",
      "Performing test digit 2-gram...\n",
      "['00', '01', '02', '03', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '30', '31', '32', '33', '40', '42', '45', '49', '50', '52', '60', '65', '70', '72', '75', '79', '80', '90', '95', '96', '97', '98', '99']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9b7c363b5845119f0a0215feb828c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 2-gram in 0.23191046714782715 seconds\n",
      "Performing test digit 3-gram...\n",
      "['000', '001', '006', '010', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '045', '080', '090', '100', '112', '120', '125', '147', '149', '150', '168', '180', '190', '195', '197', '198', '199', '200', '201', '202', '250', '300', '320', '400', '471', '500', '512', '600', '680', '700', '716', '731', '800', '804', '900', '979']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe3e44c203d4ab5a451a4edb2d86d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 3-gram in 0.21552014350891113 seconds\n",
      "Performing test punctuation 1-gram...\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '’', '“', '”']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09bdbae24b145f58ac6588768b5a0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test punctuation 1-gram in 0.2709927558898926 seconds\n",
      "Performing test punctuation 2-gram...\n",
      "['!!', '!\"', '!)', '!<', '!?', '!”', '\")', '\",', '\".', '\"?', '%.', \"'.\", '(*', '(/', '))', '),', ').', '*)', '**', '*,', '*.', ',\"', ',”', '-)', '--', '->', '.\"', '.)', '.*', '.,', '..', '.]', '.”', '/)', '//', '/?', '/]', ':)', ':-', ':/', '?!', '?\"', '?)', '??', '?”', '\\\\*', '\\\\-', '\\\\_', '](', '~~']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00665a5eb694478affda1dc3041a569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test punctuation 2-gram in 0.23576927185058594 seconds\n",
      "Performing test word 1-gram...\n",
      "[('a',), ('about',), ('all',), ('an',), ('and',), ('are',), ('as',), ('at',), ('be',), ('but',), ('can',), ('do',), ('dont',), ('for',), ('from',), ('get',), ('have',), ('i',), ('if',), ('im',), ('in',), ('is',), ('it',), ('its',), ('just',), ('like',), ('me',), ('more',), ('my',), ('not',), ('of',), ('on',), ('one',), ('or',), ('out',), ('people',), ('so',), ('some',), ('that',), ('the',), ('they',), ('this',), ('to',), ('was',), ('we',), ('what',), ('with',), ('would',), ('you',), ('your',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a514db9deb9d4d4ca4e0f603af709c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test word 1-gram in 0.2945990562438965 seconds\n",
      "Performing test POS tags 1-gram...\n",
      "[('$',), (\"''\",), (',',), ('-LRB-',), ('-RRB-',), ('.',), (':',), ('ADD',), ('AFX',), ('CC',), ('CD',), ('DT',), ('EX',), ('FW',), ('HYPH',), ('IN',), ('JJ',), ('JJR',), ('JJS',), ('LS',), ('MD',), ('NFP',), ('NN',), ('NNP',), ('NNPS',), ('NNS',), ('PDT',), ('POS',), ('PRP',), ('PRP$',), ('RB',), ('RBR',), ('RBS',), ('RP',), ('SYM',), ('TO',), ('UH',), ('VB',), ('VBD',), ('VBG',), ('VBN',), ('VBP',), ('VBZ',), ('WDT',), ('WP',), ('WP$',), ('WRB',), ('XX',), ('_SP',), ('``',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a61b1cbc334e11a57ef0b15fc23ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test POS tags 1-gram in 0.24094891548156738 seconds\n",
      "Performing test POS tags 2-gram...\n",
      "[(',', 'CC'), (',', 'PRP'), ('.', 'PRP'), ('.', 'RB'), ('CC', 'PRP'), ('DT', 'JJ'), ('DT', 'NN'), ('DT', 'NNS'), ('IN', 'DT'), ('IN', 'JJ'), ('IN', 'NN'), ('IN', 'NNP'), ('IN', 'NNS'), ('IN', 'PRP'), ('IN', 'PRP$'), ('JJ', '.'), ('JJ', 'IN'), ('JJ', 'NN'), ('JJ', 'NNS'), ('MD', 'VB'), ('NN', ','), ('NN', '.'), ('NN', 'CC'), ('NN', 'IN'), ('NN', 'NN'), ('NN', 'NNS'), ('NN', 'VBZ'), ('NNP', 'NNP'), ('NNS', '.'), ('NNS', 'IN'), ('PRP', 'MD'), ('PRP', 'RB'), ('PRP', 'VBD'), ('PRP', 'VBP'), ('PRP', 'VBZ'), ('PRP$', 'NN'), ('RB', '.'), ('RB', 'IN'), ('RB', 'JJ'), ('RB', 'RB'), ('RB', 'VB'), ('TO', 'VB'), ('VB', 'DT'), ('VB', 'IN'), ('VB', 'PRP'), ('VBN', 'IN'), ('VBP', 'DT'), ('VBP', 'RB'), ('VBZ', 'DT'), ('VBZ', 'RB')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ef23e525a14e17ba628c0fcc304b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test POS tags 2-gram in 0.3113884925842285 seconds\n",
      "Performing letter count & proportion...\n",
      "Performed letter count & proportion in 0.9183580875396729 seconds\n",
      "Performing digit count & proportion...\n",
      "Performed digit count & proportion in 0.6570332050323486 seconds\n",
      "Performing punctuation count & proportion...\n",
      "Performed punctuation count & proportion in 1.5225162506103516 seconds\n",
      "Performing whitespace count & proportion...\n",
      "Performed whitespace count & proportion in 0.5180864334106445 seconds\n",
      "Performing character count & proportion...\n",
      "Performed character count & proportion in 0.026520252227783203 seconds\n",
      "Performing word count...\n",
      "Performed word count in 0.09534502029418945 seconds\n",
      "Performing word length avg...\n",
      "Performed word length avg in 0.1456148624420166 seconds\n",
      "Performing word length distribution...\n",
      "Performed word length distribution in 0.26849365234375 seconds\n",
      "Performing word count short...\n",
      "Performed word count short in 0.0021338462829589844 seconds\n",
      "Performing letter case distribution...\n",
      "Performed letter case distribution in 0.30924320220947266 seconds\n",
      "Performing word case distribution...\n",
      "Performed word case distribution in 0.568772554397583 seconds\n",
      "Performing misspellings proportion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ee363f363441a4841d9c8f386d1daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed misspellings proportion in 1091.658107995987 seconds\n",
      "Performing stop words ratio...\n",
      "Performed stop words ratio in 1.8346738815307617 seconds\n",
      "Performing hapax legomena proportion of total tokens...\n",
      "Performed hapax legomena proportion of total tokens in 1.6661875247955322 seconds\n",
      "Performing hapax legomena proportion of unique tokens...\n",
      "Performed hapax legomena proportion of unique tokens in 1.8396039009094238 seconds\n",
      "Performing token type ratio...\n",
      "Performed token type ratio in 2.887749671936035 seconds\n",
      "Performing train letter 1-gram...\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77118ec24bf74a92930d6fba4a3fb356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 1-gram in 3.389281988143921 seconds\n",
      "Returned up to 50 most common letter 1-grams for feature extraction on test set.\n",
      "Performing train letter 2-gram...\n",
      "['al', 'an', 'ar', 'as', 'at', 'be', 'ca', 'co', 'de', 'ea', 'ed', 'el', 'en', 'er', 'es', 'ha', 'he', 'hi', 'ho', 'in', 'is', 'it', 'le', 'li', 'll', 'ly', 'me', 'nd', 'ne', 'ng', 'no', 'nt', 'of', 'om', 'on', 'or', 'ot', 'ou', 're', 'se', 'so', 'st', 'te', 'th', 'ti', 'to', 'us', 'ut', 've', 'yo']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80856ebda3414b098c8d64195dfa6a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 2-gram in 10.321114540100098 seconds\n",
      "Returned up to 50 most common letter 2-grams for feature extraction on test set.\n",
      "Performing train letter 3-gram...\n",
      "['all', 'and', 'are', 'ate', 'ati', 'ave', 'but', 'can', 'com', 'ear', 'ent', 'ere', 'ers', 'eve', 'for', 'hat', 'hav', 'her', 'hey', 'hin', 'his', 'ike', 'ing', 'ion', 'ith', 'ive', 'jus', 'lik', 'lly', 'nce', 'not', 'ome', 'one', 'oul', 'our', 'out', 'ple', 'rea', 'som', 'ter', 'tha', 'the', 'thi', 'tio', 'uld', 'use', 'ust', 'ver', 'wit', 'you']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ea45aa3d9b44e1a0b1378c7307d4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 3-gram in 171.58079552650452 seconds\n",
      "Returned up to 50 most common letter 3-grams for feature extraction on test set.\n",
      "Performing train digit 1-gram...\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71f0c4e4f9c4ba3a3c4669fbbe002cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 1-gram in 2.673332691192627 seconds\n",
      "Returned up to 50 most common digit 1-grams for feature extraction on test set.\n",
      "Performing train digit 2-gram...\n",
      "['00', '01', '02', '03', '07', '08', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '27', '28', '30', '31', '32', '33', '35', '36', '38', '40', '41', '44', '45', '50', '55', '60', '65', '66', '68', '70', '72', '75', '77', '80', '90', '93', '96', '99']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baabd97b0934a1c88b6e5250d9ffc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 2-gram in 4.678945779800415 seconds\n",
      "Returned up to 50 most common digit 2-grams for feature extraction on test set.\n",
      "Performing train digit 3-gram...\n",
      "['000', '001', '008', '010', '013', '016', '019', '020', '021', '022', '096', '100', '101', '110', '120', '121', '122', '124', '130', '144', '150', '155', '156', '160', '166', '169', '180', '190', '200', '201', '202', '209', '212', '241', '255', '300', '364', '400', '415', '444', '477', '500', '550', '600', '613', '620', '647', '700', '800', '900']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aaedb0ca6847f991adae098d110c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 3-gram in 21.024398803710938 seconds\n",
      "Returned up to 50 most common digit 3-grams for feature extraction on test set.\n",
      "Performing train punctuation 1-gram...\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '’', '“', '”']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a022d748fe25446e988af1bd9a3e7b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train punctuation 1-gram in 3.2264230251312256 seconds\n",
      "Returned up to 50 most common punctuation 1-grams for feature extraction on test set.\n",
      "Performing train punctuation 2-gram...\n",
      "['!!', '!\"', '!)', '!<', '!”', '\")', '\"*', '\",', '\".', '\"?', '%.', '&#', \"',\", \"'.\", '))', '),', ').', ');', '*\"', '*)', '**', '*,', '*.', ',\"', '.\"', '.)', '.*', '..', '.]', '.”', '/)', '//', '/?', ':(', ':)', ':/', '>!', '?!', '?\"', '?)', '??', '\\\\*', '\\\\-', '\\\\[', '\\\\]', '\\\\_', '](', '__', '”,', '”.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd87ddc1e2049bf8469b78904adc2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train punctuation 2-gram in 31.60771608352661 seconds\n",
      "Returned up to 50 most common punctuation 2-grams for feature extraction on test set.\n",
      "Performing train word 1-gram...\n",
      "[('a',), ('about',), ('all',), ('an',), ('and',), ('are',), ('as',), ('at',), ('be',), ('because',), ('but',), ('can',), ('do',), ('dont',), ('for',), ('from',), ('have',), ('he',), ('i',), ('if',), ('im',), ('in',), ('is',), ('it',), ('its',), ('just',), ('like',), ('me',), ('more',), ('my',), ('not',), ('of',), ('on',), ('one',), ('or',), ('people',), ('so',), ('that',), ('the',), ('they',), ('think',), ('this',), ('to',), ('was',), ('what',), ('when',), ('with',), ('would',), ('you',), ('your',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ec75b7dcb6408d8cceacd510123ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train word 1-gram in 78.48701810836792 seconds\n",
      "Returned up to 50 most common word1-grams for feature extraction on test set.\n",
      "Performing train POS tags 1-grams...\n",
      "[('$',), (\"''\",), (',',), ('-LRB-',), ('-RRB-',), ('.',), (':',), ('ADD',), ('AFX',), ('CC',), ('CD',), ('DT',), ('EX',), ('FW',), ('HYPH',), ('IN',), ('JJ',), ('JJR',), ('JJS',), ('LS',), ('MD',), ('NFP',), ('NN',), ('NNP',), ('NNPS',), ('NNS',), ('PDT',), ('POS',), ('PRP',), ('PRP$',), ('RB',), ('RBR',), ('RBS',), ('RP',), ('SYM',), ('TO',), ('UH',), ('VB',), ('VBD',), ('VBG',), ('VBN',), ('VBP',), ('VBZ',), ('WDT',), ('WP',), ('WP$',), ('WRB',), ('XX',), ('_SP',), ('``',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90489297370461a8b92884e1f8dc5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train POS tags 1-gram in 1.565347671508789 seconds\n",
      "Returned up to 50 most common POS tags 1-grams for feature extraction on test set.\n",
      "Performing train POS tags 2-grams...\n",
      "[(',', 'CC'), (',', 'PRP'), ('.', 'PRP'), ('CC', 'PRP'), ('DT', 'JJ'), ('DT', 'NN'), ('DT', 'NNS'), ('IN', 'DT'), ('IN', 'JJ'), ('IN', 'NN'), ('IN', 'NNP'), ('IN', 'NNS'), ('IN', 'PRP'), ('IN', 'PRP$'), ('JJ', '.'), ('JJ', 'IN'), ('JJ', 'NN'), ('JJ', 'NNS'), ('MD', 'RB'), ('MD', 'VB'), ('NN', ','), ('NN', '.'), ('NN', 'CC'), ('NN', 'IN'), ('NN', 'NN'), ('NN', 'RB'), ('NN', 'VBZ'), ('NNP', 'NNP'), ('NNS', '.'), ('NNS', 'IN'), ('PRP', 'MD'), ('PRP', 'RB'), ('PRP', 'VBD'), ('PRP', 'VBP'), ('PRP', 'VBZ'), ('PRP$', 'NN'), ('RB', '.'), ('RB', 'IN'), ('RB', 'JJ'), ('RB', 'RB'), ('RB', 'VB'), ('TO', 'VB'), ('VB', 'DT'), ('VB', 'IN'), ('VB', 'PRP'), ('VBN', 'IN'), ('VBP', 'DT'), ('VBP', 'RB'), ('VBZ', 'DT'), ('VBZ', 'RB')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7058208112c04ee5ac7229e0ab1a430b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train POS tags 2-gram in 9.770082473754883 seconds\n",
      "Returned up to 50 most common POS tags 2-grams for feature extraction on test set.\n",
      "Performing letter count & proportion...\n",
      "Performed letter count & proportion in 0.12450480461120605 seconds\n",
      "Performing digit count & proportion...\n",
      "Performed digit count & proportion in 0.07952022552490234 seconds\n",
      "Performing punctuation count & proportion...\n",
      "Performed punctuation count & proportion in 0.15365338325500488 seconds\n",
      "Performing whitespace count & proportion...\n",
      "Performed whitespace count & proportion in 0.05900430679321289 seconds\n",
      "Performing character count & proportion...\n",
      "Performed character count & proportion in 0.006491661071777344 seconds\n",
      "Performing word count...\n",
      "Performed word count in 0.010852575302124023 seconds\n",
      "Performing word length avg...\n",
      "Performed word length avg in 0.01452183723449707 seconds\n",
      "Performing word length distribution...\n",
      "Performed word length distribution in 0.028422832489013672 seconds\n",
      "Performing word count short...\n",
      "Performed word count short in 0.0017621517181396484 seconds\n",
      "Performing letter case distribution...\n",
      "Performed letter case distribution in 0.0324711799621582 seconds\n",
      "Performing word case distribution...\n",
      "Performed word case distribution in 0.058969736099243164 seconds\n",
      "Performing misspellings proportion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee0f62199e84ace95b8ff49f47bb1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed misspellings proportion in 144.77538108825684 seconds\n",
      "Performing stop words ratio...\n",
      "Performed stop words ratio in 0.2167818546295166 seconds\n",
      "Performing hapax legomena proportion of total tokens...\n",
      "Performed hapax legomena proportion of total tokens in 0.20307278633117676 seconds\n",
      "Performing hapax legomena proportion of unique tokens...\n",
      "Performed hapax legomena proportion of unique tokens in 0.17252731323242188 seconds\n",
      "Performing token type ratio...\n",
      "Performed token type ratio in 1.1249175071716309 seconds\n",
      "Performing test letter 1-gram...\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1ba1875f5d499e8ababd40189656c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 1-gram in 0.2111964225769043 seconds\n",
      "Performing test letter 2-gram...\n",
      "['al', 'an', 'ar', 'as', 'at', 'be', 'ca', 'co', 'de', 'ea', 'ed', 'el', 'en', 'er', 'es', 'ha', 'he', 'hi', 'ho', 'in', 'is', 'it', 'le', 'li', 'll', 'ly', 'me', 'nd', 'ne', 'ng', 'no', 'nt', 'of', 'om', 'on', 'or', 'ot', 'ou', 're', 'se', 'so', 'st', 'te', 'th', 'ti', 'to', 'us', 'ut', 've', 'yo']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b2013a18754dae953ee7debe7da51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 2-gram in 0.23292851448059082 seconds\n",
      "Performing test letter 3-gram...\n",
      "['all', 'and', 'are', 'ate', 'ati', 'ave', 'but', 'can', 'com', 'ear', 'ent', 'ere', 'ers', 'eve', 'for', 'hat', 'hav', 'her', 'hey', 'hin', 'his', 'ike', 'ing', 'ion', 'ith', 'ive', 'jus', 'lik', 'lly', 'nce', 'not', 'ome', 'one', 'oul', 'our', 'out', 'ple', 'rea', 'som', 'ter', 'tha', 'the', 'thi', 'tio', 'uld', 'use', 'ust', 'ver', 'wit', 'you']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8759059a2c944307977a97586c359034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 3-gram in 0.20315051078796387 seconds\n",
      "Performing test digit 1-gram...\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e49cc8c6934a9f9611685129cd6519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 1-gram in 0.2618598937988281 seconds\n",
      "Performing test digit 2-gram...\n",
      "['00', '01', '02', '03', '07', '08', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '27', '28', '30', '31', '32', '33', '35', '36', '38', '40', '41', '44', '45', '50', '55', '60', '65', '66', '68', '70', '72', '75', '77', '80', '90', '93', '96', '99']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa4676dad5a4b6fb1fa27aa557f0ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 2-gram in 0.28728270530700684 seconds\n",
      "Performing test digit 3-gram...\n",
      "['000', '001', '008', '010', '013', '016', '019', '020', '021', '022', '096', '100', '101', '110', '120', '121', '122', '124', '130', '144', '150', '155', '156', '160', '166', '169', '180', '190', '200', '201', '202', '209', '212', '241', '255', '300', '364', '400', '415', '444', '477', '500', '550', '600', '613', '620', '647', '700', '800', '900']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f323c6bd9cf474e9a13573c2c25301a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 3-gram in 0.20129919052124023 seconds\n",
      "Performing test punctuation 1-gram...\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '’', '“', '”']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbc3451385345f0891592fd4601c2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test punctuation 1-gram in 0.23924636840820312 seconds\n",
      "Performing test punctuation 2-gram...\n",
      "['!!', '!\"', '!)', '!<', '!”', '\")', '\"*', '\",', '\".', '\"?', '%.', '&#', \"',\", \"'.\", '))', '),', ').', ');', '*\"', '*)', '**', '*,', '*.', ',\"', '.\"', '.)', '.*', '..', '.]', '.”', '/)', '//', '/?', ':(', ':)', ':/', '>!', '?!', '?\"', '?)', '??', '\\\\*', '\\\\-', '\\\\[', '\\\\]', '\\\\_', '](', '__', '”,', '”.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21742e7fb15147d6bf224ccea56a8ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test punctuation 2-gram in 0.21889901161193848 seconds\n",
      "Performing test word 1-gram...\n",
      "[('a',), ('about',), ('all',), ('an',), ('and',), ('are',), ('as',), ('at',), ('be',), ('because',), ('but',), ('can',), ('do',), ('dont',), ('for',), ('from',), ('have',), ('he',), ('i',), ('if',), ('im',), ('in',), ('is',), ('it',), ('its',), ('just',), ('like',), ('me',), ('more',), ('my',), ('not',), ('of',), ('on',), ('one',), ('or',), ('people',), ('so',), ('that',), ('the',), ('they',), ('think',), ('this',), ('to',), ('was',), ('what',), ('when',), ('with',), ('would',), ('you',), ('your',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357ac292530e4bc9a38d03fe0e02aaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test word 1-gram in 0.26030802726745605 seconds\n",
      "Performing test POS tags 1-gram...\n",
      "[('$',), (\"''\",), (',',), ('-LRB-',), ('-RRB-',), ('.',), (':',), ('ADD',), ('AFX',), ('CC',), ('CD',), ('DT',), ('EX',), ('FW',), ('HYPH',), ('IN',), ('JJ',), ('JJR',), ('JJS',), ('LS',), ('MD',), ('NFP',), ('NN',), ('NNP',), ('NNPS',), ('NNS',), ('PDT',), ('POS',), ('PRP',), ('PRP$',), ('RB',), ('RBR',), ('RBS',), ('RP',), ('SYM',), ('TO',), ('UH',), ('VB',), ('VBD',), ('VBG',), ('VBN',), ('VBP',), ('VBZ',), ('WDT',), ('WP',), ('WP$',), ('WRB',), ('XX',), ('_SP',), ('``',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a2aaed5108488a8d7ccd03a4053150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test POS tags 1-gram in 0.20082640647888184 seconds\n",
      "Performing test POS tags 2-gram...\n",
      "[(',', 'CC'), (',', 'PRP'), ('.', 'PRP'), ('CC', 'PRP'), ('DT', 'JJ'), ('DT', 'NN'), ('DT', 'NNS'), ('IN', 'DT'), ('IN', 'JJ'), ('IN', 'NN'), ('IN', 'NNP'), ('IN', 'NNS'), ('IN', 'PRP'), ('IN', 'PRP$'), ('JJ', '.'), ('JJ', 'IN'), ('JJ', 'NN'), ('JJ', 'NNS'), ('MD', 'RB'), ('MD', 'VB'), ('NN', ','), ('NN', '.'), ('NN', 'CC'), ('NN', 'IN'), ('NN', 'NN'), ('NN', 'RB'), ('NN', 'VBZ'), ('NNP', 'NNP'), ('NNS', '.'), ('NNS', 'IN'), ('PRP', 'MD'), ('PRP', 'RB'), ('PRP', 'VBD'), ('PRP', 'VBP'), ('PRP', 'VBZ'), ('PRP$', 'NN'), ('RB', '.'), ('RB', 'IN'), ('RB', 'JJ'), ('RB', 'RB'), ('RB', 'VB'), ('TO', 'VB'), ('VB', 'DT'), ('VB', 'IN'), ('VB', 'PRP'), ('VBN', 'IN'), ('VBP', 'DT'), ('VBP', 'RB'), ('VBZ', 'DT'), ('VBZ', 'RB')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b79e5637bc4d7391cf317e23c3d51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test POS tags 2-gram in 0.32909536361694336 seconds\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "team_seed = 13 + 4 + 5\n",
    "\n",
    "def extract_features(cohort, filetag):\n",
    "    \n",
    "    y = cohort[['author_id', 'intra_author_feed_id']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(cohort, y, test_size=0.10, stratify = y['author_id'], random_state=team_seed)\n",
    "\n",
    "    for stage in [\"train\", \"test\"]:\n",
    "        if stage == \"train\":\n",
    "            feeds_aug = X_train\n",
    "        elif stage == \"test\":\n",
    "            feeds_aug = X_test\n",
    "            \n",
    "        # Number of Comments, Median comment length\n",
    "\n",
    "        feeds_aug['num_comments'] = feeds_aug['comment_lengths'].apply(len)\n",
    "        feeds_aug['comment_length_median'] = feeds_aug['comment_lengths'].apply(statistics.median)\n",
    "        feeds_aug = feeds_aug.drop('comment_lengths', axis=1)\n",
    "\n",
    "        # Character Count, Alphabet Count & Proportion, Digit Count & Proportion, Punctuation Count & Proportion\n",
    "\n",
    "        feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'letter')\n",
    "        feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'digit')\n",
    "        feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'punctuation')\n",
    "        feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'whitespace')\n",
    "        feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'character')\n",
    "    \n",
    "        # Word Count, Average Word Length, Word Length Distribution (Freq of words length 1-20 letters), Word Case Distribution (All lowercase / First-upper-rest-lowercase / All uppercase / Other), Character case distribution (lowercase / uppercase)\n",
    "\n",
    "        word_count_wrapper(feeds_aug, 'feed_string')\n",
    "        word_length_avg_wrapper(feeds_aug, 'feed_string')\n",
    "        word_length_distribution_wrapper(feeds_aug, 'feed_string')\n",
    "        word_short_prop_wrapper(feeds_aug, 'word_length_distribution', 'word_count')\n",
    "        letter_case_distribution_wrapper(feeds_aug, 'feed_string')\n",
    "        word_case_distribution_wrapper(feeds_aug, 'feed_string')\n",
    "\n",
    "        # Misspellings Prop\n",
    "\n",
    "        misspelled_prop_wrapper(feeds_aug, 'feed_string', 'misspelled_prop')\n",
    "    \n",
    "        #Stop Word proportion of Tokens\n",
    "        stop_words_proportion_wrapper(feeds_aug, 'feed_comment_list_spacy', 'stop_words_proportion')\n",
    "    \n",
    "        # Vocabulary Richness: Hapax Legomena Proportion of Total Tokens, Hapax Legomena Proportion of Unique Tokens, Unique Tokens over Total Tokens\n",
    "        # # https://eprints.qut.edu.au/8019/1/8019.pdf\n",
    "    \n",
    "        hapax_legomena_proportion_wrapper(feeds_aug, 'feed_comment_list_spacy', 'hapax_legomena_prop_tot_tokens', 'total') # Note: ignores stop words\n",
    "        hapax_legomena_proportion_wrapper(feeds_aug, 'feed_comment_list_spacy', 'hapax_legomena_prop_unique_tokens', 'unique') # Note: ignores stop words\n",
    "        token_type_ratio_wrapper(feeds_aug, 'feed_comment_list_spacy', 'token_type_ratio') # Note: ignores stop words\n",
    "\n",
    "        if stage == \"train\":\n",
    "        \n",
    "            global letter_1gram_collection_fromtrain\n",
    "            global letter_2gram_collection_fromtrain\n",
    "            global letter_3gram_collection_fromtrain\n",
    "            global letter_4gram_collection_fromtrain\n",
    "            global digit_1gram_collection_fromtrain\n",
    "            global digit_2gram_collection_fromtrain\n",
    "            global digit_3gram_collection_fromtrain\n",
    "            global punctuation_1gram_collection_fromtrain\n",
    "            global punctuation_2gram_collection_fromtrain\n",
    "            global punctuation_3gram_collection_fromtrain\n",
    "            global word_1gram_collection_fromtrain\n",
    "            global word_2gram_collection_fromtrain\n",
    "            global POS_tags_1gram_collection_fromtrain\n",
    "            global POS_tags_2gram_collection_fromtrain\n",
    "            global POS_tags_3gram_collection_fromtrain\n",
    "        \n",
    "            # Letter, Digit, and Punctuation n-grams\n",
    "            letter_1gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_1gram', 1, 50, 'letter')\n",
    "            letter_2gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_2gram', 2, 50, 'letter')\n",
    "            letter_3gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_3gram', 3, 50, 'letter')\n",
    "            letter_4gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_4gram', 4, 50, 'letter')\n",
    "\n",
    "            digit_1gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_1gram', 1, 50, 'digit')\n",
    "            #digit_2gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_2gram', 2, 50, 'digit')\n",
    "            #digit_3gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_3gram', 3, 50, 'digit')\n",
    "\n",
    "            punctuation_1gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_1gram', 1, 50, 'punctuation')\n",
    "            punctuation_2gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_2gram', 2, 50, 'punctuation')\n",
    "            #punctuation_3gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_3gram', 3, 50, 'punctuation')\n",
    "\n",
    "            # Word ngrams\n",
    "            word_1gram_collection_fromtrain = word_ngrams_wrapper(feeds_aug, 'feed_comment_list_nopunc_lower', 'word_1gram', 1, 50)\n",
    "            word_2gram_collection_fromtrain = word_ngrams_wrapper(feeds_aug, 'feed_comment_list_nopunc_lower', 'word_1gram', 2, 50)\n",
    "\n",
    "            # POS n-grams\n",
    "            POS_tags_1gram_collection_fromtrain = POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_1gram', 1, 50)\n",
    "            POS_tags_2gram_collection_fromtrain = POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_2gram', 2, 50)\n",
    "            #POS_tags_3gram_collection_fromtrain = POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_3gram', 3, 50)\n",
    "\n",
    "        elif stage == \"test\":\n",
    "        \n",
    "            # Letter, Digit, and Punctuation n-grams\n",
    "            character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_1gram', 1, 50, 'letter', letter_1gram_collection_fromtrain)\n",
    "            character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_2gram', 2, 50, 'letter', letter_2gram_collection_fromtrain)\n",
    "            character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_3gram', 3, 50, 'letter', letter_3gram_collection_fromtrain)\n",
    "            character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_4gram', 4, 50, 'letter', letter_4gram_collection_fromtrain)\n",
    "\n",
    "            character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_1gram', 1, 50, 'digit', digit_1gram_collection_fromtrain)\n",
    "            #character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_2gram', 2, 50, 'digit', digit_2gram_collection_fromtrain)\n",
    "            #character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_3gram', 3, 50, 'digit', digit_3gram_collection_fromtrain)\n",
    "\n",
    "            character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_1gram', 1, 50, 'punctuation', punctuation_1gram_collection_fromtrain)\n",
    "            character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_2gram', 2, 50, 'punctuation', punctuation_2gram_collection_fromtrain)\n",
    "            #character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_3gram', 3, 50, 'punctuation', punctuation_3gram_collection_fromtrain)\n",
    "\n",
    "            # Word ngrams\n",
    "            word_ngrams_wrapper(feeds_aug, 'feed_comment_list_nopunc_lower', 'word_1gram', 1, 50, word_1gram_collection_fromtrain)\n",
    "            word_ngrams_wrapper(feeds_aug, 'feed_comment_list_nopunc_lower', 'word_1gram', 2, 50, word_2gram_collection_fromtrain)\n",
    "\n",
    "            # POS n-grams\n",
    "            POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_1gram', 1, 50, POS_tags_1gram_collection_fromtrain)\n",
    "            POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_2gram', 2, 50, POS_tags_2gram_collection_fromtrain)\n",
    "            #POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_3gram', 3, 50, POS_tags_3gram_collection_fromtrain)\n",
    "\n",
    "        # IMPORTANT: If any features are commented-in above, they must be added to the feature list in this next line\n",
    "        feeds_aug = feeds_aug[['proficiency', 'comment_length_median', 'letter_prop', 'digit_prop', 'punctuation_prop', 'whitespace_prop', 'word_length_avg', 'word_length_distribution', 'word_short_prop', 'letter_case_distribution', 'word_case_distribution', 'misspelled_prop', 'stop_words_proportion', 'hapax_legomena_prop_tot_tokens', 'hapax_legomena_prop_unique_tokens', 'token_type_ratio', 'letter_1gram', 'letter_2gram', 'letter_3gram', 'letter_4gram', 'digit_1gram', 'punctuation_1gram', 'punctuation_2gram', 'word_1gram', 'word_2gram', 'POS_tag_1gram', 'POS_tag_2gram']]\n",
    "        for col in feeds_aug.columns:\n",
    "            if type(feeds_aug[col].iloc[0]) == list:\n",
    "                newcols = [col + \"_\" + str(i) for i in range(1,len(feeds_aug[col].iloc[0]) + 1)]\n",
    "                feeds_aug[newcols] = pd.DataFrame(feeds_aug[col].tolist(), index= feeds_aug.index)\n",
    "                feeds_aug = feeds_aug.drop([col], axis = 1)\n",
    "            elif type(feeds_aug[col].iloc[0]) == str and col != 'proficiency':\n",
    "                feeds_aug[col] = pd.to_numeric(feeds_aug[col], downcast=\"float\")\n",
    "            \n",
    "        if stage == \"train\":\n",
    "            X_train = feeds_aug\n",
    "        elif stage == \"test\":\n",
    "            X_test = feeds_aug\n",
    "        \n",
    "    X_train.to_pickle(\"dev_\" + filetag + \"_X_train.pkl\")\n",
    "    X_test.to_pickle(\"dev_\" + filetag + \"_X_test.pkl\")\n",
    "    y_train.to_pickle(\"dev_\" + filetag + \"_y_train.pkl\")\n",
    "    y_test.to_pickle(\"dev_\" + filetag + \"_y_test.pkl\")\n",
    "\n",
    "extract_features(cohort_all, \"cohort_all\")\n",
    "extract_features(cohort_native, \"cohort_native\")\n",
    "extract_features(cohort_nonnat, \"cohort_nonnat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4739e0-a2d0-4f71-b323-b4dc398acc50",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4b355-64b4-40e7-a85d-91b546e9840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.baeldung.com/cs/svm-multiclass-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6f939-5ec4-42fc-8ba5-08450dd5ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM linear kernel results for cohort_all:\n",
      " \n",
      "- Accuracy\n",
      "-- Overall:  50.00\n",
      "-- Native: 46.67\n",
      "-- Non-Native: 53.33\n",
      " \n",
      "- F1 Score\n",
      "-- Overall: 47.73\n",
      "-- Native: 51.33\n",
      "-- Non-Native: 58.89\n",
      " \n",
      " \n",
      "Results are for 15 native 15 non-native english authors. Model trained on 540 feeds and tested on 60 feeds.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "SVM linear kernel results for cohort_native:\n",
      " \n",
      "- Accuracy\n",
      "-- Overall:  55.00\n",
      "-- Native: 55.00\n",
      "-- Non-Native: 0.00\n",
      " \n",
      "- F1 Score\n",
      "-- Overall: 49.02\n",
      "-- Native: 49.02\n",
      "-- Non-Native: 0.00\n",
      " \n",
      " \n",
      "Results are for 30 native 0 non-native english authors. Model trained on 540 feeds and tested on 60 feeds.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "SVM linear kernel results for cohort_nonnat:\n",
      " \n",
      "- Accuracy\n",
      "-- Overall:  48.33\n",
      "-- Native: 0.00\n",
      "-- Non-Native: 48.33\n",
      " \n",
      "- F1 Score\n",
      "-- Overall: 46.56\n",
      "-- Native: 0.00\n",
      "-- Non-Native: 46.56\n",
      " \n",
      " \n",
      "Results are for 0 native 30 non-native english authors. Model trained on 540 feeds and tested on 60 feeds.\n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "def classify(filetag, kernel):\n",
    "    # Import train and test data w features. Split out proficiency.\n",
    "    X_train = pd.read_pickle('dev_' + filetag + '_X_train.pkl')\n",
    "    train_proficiency = X_train['proficiency']\n",
    "    X_train = X_train.drop(['proficiency'], axis = 1)\n",
    "\n",
    "    X_test = pd.read_pickle('dev_' + filetag + '_X_test.pkl')\n",
    "    test_proficiency = X_test['proficiency']\n",
    "    X_test = X_test.drop(['proficiency'], axis = 1)\n",
    "    \n",
    "    y_train = pd.read_pickle('dev_' + filetag + '_y_train.pkl')\n",
    "    y_test = pd.read_pickle('dev_' + filetag + '_y_test.pkl')\n",
    "\n",
    "    # Train SVM models - separate if statements because gamma and C can differ. These numbers simply taken from article; could be tuned.\n",
    "    if kernel == \"rbf\":\n",
    "        model = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(X_train, y_train['author_id'])\n",
    "    if kernel == \"poly\":\n",
    "        model = svm.SVC(kernel='poly', degree=3, C=1).fit(X_train, y_train['author_id'])\n",
    "    if kernel == \"linear\":\n",
    "        model = svm.SVC(kernel='linear', degree=3, C=1).fit(X_train, y_train['author_id'])\n",
    "    \n",
    "    # Split test into native and nonnative feeds (can handle all-native or all-non-native set)\n",
    "    X_test_all_proficiencies = X_test.merge(test_proficiency, left_index=True, right_index=True, how='inner')\n",
    "    \n",
    "    X_test_native = X_test_all_proficiencies.loc[X_test_all_proficiencies['proficiency'] == \"N\"]\n",
    "    y_test_native = y_test['author_id'].loc[X_test_all_proficiencies['proficiency'] == \"N\"]\n",
    "    X_test_native = X_test_native.drop(['proficiency'], axis = 1)\n",
    "    \n",
    "    X_test_nonnat = X_test_all_proficiencies.loc[X_test_all_proficiencies['proficiency'] == \"L\"]\n",
    "    y_test_nonnat = y_test['author_id'].loc[X_test_all_proficiencies['proficiency'] == \"L\"]\n",
    "    X_test_nonnat = X_test_nonnat.drop(['proficiency'], axis = 1)\n",
    "    \n",
    "    # Predict native and non-native\n",
    "    pred_all = model.predict(X_test)\n",
    "    accuracy_all = accuracy_score(y_test['author_id'], pred_all)\n",
    "    f1_all = f1_score(y_test['author_id'], pred_all, average='weighted')\n",
    "        \n",
    "    if X_test_native.shape[0] > 0:\n",
    "        pred_native = model.predict(X_test_native)\n",
    "        accuracy_native = accuracy_score(y_test_native, pred_native)\n",
    "        f1_native = f1_score(y_test_native, pred_native, average='weighted')\n",
    "    else:\n",
    "        accuracy_native = 0\n",
    "        f1_native = 0\n",
    "        \n",
    "    if X_test_nonnat.shape[0] > 0:\n",
    "        pred_nonnat = model.predict(X_test_nonnat)\n",
    "        accuracy_nonnat = accuracy_score(y_test_nonnat, pred_nonnat)\n",
    "        f1_nonnat = f1_score(y_test_nonnat, pred_nonnat, average='weighted')\n",
    "    else:\n",
    "        accuracy_nonnat = 0\n",
    "        f1_nonnat = 0\n",
    "            \n",
    "    print('SVM ' + model.kernel + ' kernel results for ' + filetag + ':')\n",
    "    print(' ')\n",
    "    print('- Accuracy')\n",
    "    print('-- Overall: ', \"%.2f\" % (accuracy_all*100))\n",
    "    print('-- Native:', \"%.2f\" % (accuracy_native*100))\n",
    "    print('-- Non-Native:', \"%.2f\" % (accuracy_nonnat*100))\n",
    "    print(' ')\n",
    "    print('- F1 Score')\n",
    "    print('-- Overall:', \"%.2f\" % (f1_all*100))\n",
    "    print('-- Native:', \"%.2f\" % (f1_native*100))\n",
    "    print('-- Non-Native:', \"%.2f\" % (f1_nonnat*100))\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print('Results are for ' + str(len(set(y_test_native))) + ' native ' + str(len(set(y_test_nonnat))) + ' non-native english authors. Model trained on ' + str(train_proficiency.shape[0]) + ' feeds and tested on ' + str(test_proficiency.shape[0]) + ' feeds.') \n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    \n",
    "classify(\"cohort_all\", \"linear\")\n",
    "classify(\"cohort_native\", \"linear\")\n",
    "classify(\"cohort_nonnat\", \"linear\")\n",
    "\n",
    "#classify(\"cohort_all\", \"rbf\")\n",
    "#classify(\"cohort_native\", \"rbf\")\n",
    "#classify(\"cohort_nonnat\", \"rbf\")\n",
    "\n",
    "#classify(\"cohort_all\", \"poly\")\n",
    "#classify(\"cohort_native\", \"poly\")\n",
    "#classify(\"cohort_nonnat\", \"poly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1 (main, Jan 24 2022, 23:33:09) [Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "63412e708ced85145ee3358b7b7540b5b7bcbe0d3c78f470347fabcaf1a15eaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
