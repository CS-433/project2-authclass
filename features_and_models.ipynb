{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419700b3-7581-42ae-80a6-4c11a8c8d8c2",
   "metadata": {},
   "source": [
    "# Sample Authors and Prepare Feeds for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c1306dc-fecc-4b4b-9216-9019f77d51f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing feeds...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8048feeb0234afab0ff5dee8cb27b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeds tokenized in 273.94717597961426 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timerange</th>\n",
       "      <th>proficiency</th>\n",
       "      <th>feed_tokens_space</th>\n",
       "      <th>comment_lengths</th>\n",
       "      <th>feed_string</th>\n",
       "      <th>feed_comment_list</th>\n",
       "      <th>feed_comment_list_nopunc_lower</th>\n",
       "      <th>feed_comment_list_spacy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th>intra_author_feed_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Brad_Ethan</th>\n",
       "      <th>1</th>\n",
       "      <td>270</td>\n",
       "      <td>N</td>\n",
       "      <td>[917, right, swipes, and, 119, matches, while,...</td>\n",
       "      <td>[12, 17, 34, 15, 13, 15, 8, 96, 27, 19, 26, 12...</td>\n",
       "      <td>917 right swipes and 119 matches while being t...</td>\n",
       "      <td>[[917, right, swipes, and, 119, matches, while...</td>\n",
       "      <td>[[917, right, swipes, and, 119, matches, while...</td>\n",
       "      <td>[(917, right, swipes, and, 119, matches, while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>270</td>\n",
       "      <td>N</td>\n",
       "      <td>[I, don't, get, why, move, on, from, Wentz...,...</td>\n",
       "      <td>[62, 24, 80, 85, 15, 18, 25, 112, 17, 6, 22, 36]</td>\n",
       "      <td>I don't get why move on from Wentz... Replace ...</td>\n",
       "      <td>[[I, don't, get, why, move, on, from, Wentz......</td>\n",
       "      <td>[[i, dont, get, why, move, on, from, wentz, re...</td>\n",
       "      <td>[(I, do, n't, get, why, move, on, from, Wentz,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>270</td>\n",
       "      <td>N</td>\n",
       "      <td>[It's, because, we, are, wasting, roster, spot...</td>\n",
       "      <td>[11, 44, 11, 5, 24, 12, 12, 44, 37, 18, 39, 7,...</td>\n",
       "      <td>It's because we are wasting roster spots with ...</td>\n",
       "      <td>[[It's, because, we, are, wasting, roster, spo...</td>\n",
       "      <td>[[its, because, we, are, wasting, roster, spot...</td>\n",
       "      <td>[(It, 's, because, we, are, wasting, roster, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>N</td>\n",
       "      <td>[I, mean, we, could, keep, Fischer, and, roll,...</td>\n",
       "      <td>[89, 38, 48, 66, 61, 22, 19, 30, 14, 17, 10, 4...</td>\n",
       "      <td>I mean we could keep Fischer and roll with Fis...</td>\n",
       "      <td>[[I, mean, we, could, keep, Fischer, and, roll...</td>\n",
       "      <td>[[i, mean, we, could, keep, fischer, and, roll...</td>\n",
       "      <td>[(I, mean, we, could, keep, Fischer, and, roll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>270</td>\n",
       "      <td>N</td>\n",
       "      <td>[I, don't, think, Skip, actually, means, it., ...</td>\n",
       "      <td>[43, 4, 17, 39, 13, 11, 30, 41, 14, 46, 11, 23...</td>\n",
       "      <td>I don't think Skip actually means it. Isn't hi...</td>\n",
       "      <td>[[I, don't, think, Skip, actually, means, it.,...</td>\n",
       "      <td>[[i, dont, think, skip, actually, means, it, i...</td>\n",
       "      <td>[(I, do, n't, think, Skip, actually, means, it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 timerange proficiency  \\\n",
       "author     intra_author_feed_id                          \n",
       "Brad_Ethan 1                           270           N   \n",
       "           2                           270           N   \n",
       "           3                           270           N   \n",
       "           4                           270           N   \n",
       "           5                           270           N   \n",
       "\n",
       "                                                                 feed_tokens_space  \\\n",
       "author     intra_author_feed_id                                                      \n",
       "Brad_Ethan 1                     [917, right, swipes, and, 119, matches, while,...   \n",
       "           2                     [I, don't, get, why, move, on, from, Wentz...,...   \n",
       "           3                     [It's, because, we, are, wasting, roster, spot...   \n",
       "           4                     [I, mean, we, could, keep, Fischer, and, roll,...   \n",
       "           5                     [I, don't, think, Skip, actually, means, it., ...   \n",
       "\n",
       "                                                                   comment_lengths  \\\n",
       "author     intra_author_feed_id                                                      \n",
       "Brad_Ethan 1                     [12, 17, 34, 15, 13, 15, 8, 96, 27, 19, 26, 12...   \n",
       "           2                      [62, 24, 80, 85, 15, 18, 25, 112, 17, 6, 22, 36]   \n",
       "           3                     [11, 44, 11, 5, 24, 12, 12, 44, 37, 18, 39, 7,...   \n",
       "           4                     [89, 38, 48, 66, 61, 22, 19, 30, 14, 17, 10, 4...   \n",
       "           5                     [43, 4, 17, 39, 13, 11, 30, 41, 14, 46, 11, 23...   \n",
       "\n",
       "                                                                       feed_string  \\\n",
       "author     intra_author_feed_id                                                      \n",
       "Brad_Ethan 1                     917 right swipes and 119 matches while being t...   \n",
       "           2                     I don't get why move on from Wentz... Replace ...   \n",
       "           3                     It's because we are wasting roster spots with ...   \n",
       "           4                     I mean we could keep Fischer and roll with Fis...   \n",
       "           5                     I don't think Skip actually means it. Isn't hi...   \n",
       "\n",
       "                                                                 feed_comment_list  \\\n",
       "author     intra_author_feed_id                                                      \n",
       "Brad_Ethan 1                     [[917, right, swipes, and, 119, matches, while...   \n",
       "           2                     [[I, don't, get, why, move, on, from, Wentz......   \n",
       "           3                     [[It's, because, we, are, wasting, roster, spo...   \n",
       "           4                     [[I, mean, we, could, keep, Fischer, and, roll...   \n",
       "           5                     [[I, don't, think, Skip, actually, means, it.,...   \n",
       "\n",
       "                                                    feed_comment_list_nopunc_lower  \\\n",
       "author     intra_author_feed_id                                                      \n",
       "Brad_Ethan 1                     [[917, right, swipes, and, 119, matches, while...   \n",
       "           2                     [[i, dont, get, why, move, on, from, wentz, re...   \n",
       "           3                     [[its, because, we, are, wasting, roster, spot...   \n",
       "           4                     [[i, mean, we, could, keep, fischer, and, roll...   \n",
       "           5                     [[i, dont, think, skip, actually, means, it, i...   \n",
       "\n",
       "                                                           feed_comment_list_spacy  \n",
       "author     intra_author_feed_id                                                     \n",
       "Brad_Ethan 1                     [(917, right, swipes, and, 119, matches, while...  \n",
       "           2                     [(I, do, n't, get, why, move, on, from, Wentz,...  \n",
       "           3                     [(It, 's, because, we, are, wasting, roster, s...  \n",
       "           4                     [(I, mean, we, could, keep, Fischer, and, roll...  \n",
       "           5                     [(I, do, n't, think, Skip, actually, means, it...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from feature_extraction_functions import *\n",
    "team_seed = 13 + 4 + 5\n",
    "random.seed(team_seed) \n",
    "\n",
    "num_native_authors_to_sample = 15\n",
    "num_nonnat_authors_to_sample = 15\n",
    "\n",
    "# Space-tokenized, long-chain version of feeds (Arthur's version + some cleaning and wide-to-long conversion)\n",
    "eng_nonnat = pd.read_pickle('Data/Classified/non_native_english_40feeds')\n",
    "eng_nat = pd.read_pickle('Data/Classified/native_english_40feeds')\n",
    "eng_nonnat['proficiency'] = \"N\" # native\n",
    "eng_nat['proficiency'] = \"L\" # learner\n",
    "# if we want a more uniform timerange, we can keep if timerange < x here\n",
    "eng_nonnat_sample = eng_nonnat.sample(num_nonnat_authors_to_sample, random_state=team_seed)\n",
    "eng_native_sample = eng_nat.sample(num_native_authors_to_sample, random_state=team_seed)\n",
    "eng_feeds = pd.concat([eng_nonnat_sample, eng_native_sample], ignore_index=False, axis=0)\n",
    "eng_feeds['author'] = eng_feeds.index\n",
    "eng_feeds = pd.wide_to_long(eng_feeds, [\"file\", \"slices\"], i=\"author\", j=\"intra_author_feed_id\").sort_index()\n",
    "eng_feeds = eng_feeds.rename(columns={\"slices\": \"comment_lengths\", \"file\": \"feed_tokens_space\"})\n",
    "\n",
    "# String version of feeds\n",
    "def feed_string(feed):\n",
    "    return ' '.join(feed)\n",
    "eng_feeds['feed_string'] = eng_feeds['feed_tokens_space'].apply(feed_string)\n",
    "\n",
    "# List-of-comments version of feeds\n",
    "## Doing this via loop because two columns involved in function instead of one...\n",
    "def create_comment_word_indices(comment_lengths):\n",
    "    np_comment_lengths = np.array(comment_lengths)\n",
    "    return np.cumsum(np_comment_lengths)\n",
    "eng_feeds['comment_word_indices'] = eng_feeds['comment_lengths'].apply(create_comment_word_indices)\n",
    "\n",
    "eng_feeds['feed_comment_list'] = \"\"\n",
    "for index, row in eng_feeds.iterrows():\n",
    "    comm_w_indices_temp = row['comment_word_indices']\n",
    "    feed_tokens_space_temp = row['feed_tokens_space']\n",
    "    inner_list = []\n",
    "    for i in range(len(comm_w_indices_temp)):\n",
    "        if i == 0:\n",
    "            inner_list.append(feed_tokens_space_temp[0:comm_w_indices_temp[i]])\n",
    "        else:\n",
    "            inner_list.append(feed_tokens_space_temp[comm_w_indices_temp[i-1]:comm_w_indices_temp[i]])\n",
    "    eng_feeds.at[index,'feed_comment_list'] = inner_list\n",
    "eng_feeds = eng_feeds.drop('comment_word_indices', axis=1)\n",
    "\n",
    "# List-of-comments w/ punctuation stripped and lowercase applied version of feeds\n",
    "def strip_punc_and_lower_nested_list(feed_comment_list):\n",
    "    feed_comment_list_nopunc_lower = []\n",
    "    for comment in feed_comment_list:\n",
    "        feed_comment_list_nopunc_lower.append(re.sub(r'[^A-Za-z0-9 ]+', '', ' '.join(comment)).lower().split()) \n",
    "    return feed_comment_list_nopunc_lower\n",
    "eng_feeds['feed_comment_list_nopunc_lower'] = eng_feeds['feed_comment_list'].apply(strip_punc_and_lower_nested_list)\n",
    "\n",
    "# List-of-comments Spacy-tokenized version of feeds\n",
    "tokenizer_wrapper(eng_feeds, 'feed_comment_list')\n",
    "\n",
    "eng_feeds.to_pickle(\"eng_feeds_pre_split.pkl\")\n",
    "\n",
    "eng_feeds.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cbbeec-dd40-4f85-8c91-e761724ba311",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebce6f7a-9e15-4ab8-ac2a-10d53ef93a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction_functions import *\n",
    "team_seed = 13 + 4 + 5\n",
    "random.seed(team_seed) \n",
    "\n",
    "eng_feeds = pd.read_pickle(\"eng_feeds_pre_split.pkl\")\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "eng_feeds = eng_feeds.reset_index()\n",
    "\n",
    "t = eng_feeds['author']\n",
    "t = labelencoder.fit_transform(t)\n",
    "eng_feeds['author_id'] = t.tolist()\n",
    "\n",
    "t = eng_feeds['proficiency']\n",
    "t = labelencoder.fit_transform(t)\n",
    "eng_feeds['proficiency_id'] = t.tolist()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = eng_feeds[['author_id', 'intra_author_feed_id']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(eng_feeds, y, test_size=0.10, random_state=team_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d07203-7b2d-408d-8119-d8d77af63a33",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856b62e6-6eae-48a2-8ef4-650e3a8b3933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing letter count & proportion...\n",
      "Performed letter count & proportion in 3.086843490600586 seconds\n",
      "Performing digit count & proportion...\n",
      "Performed digit count & proportion in 1.0787100791931152 seconds\n",
      "Performing punctuation count & proportion...\n",
      "Performed punctuation count & proportion in 2.15299391746521 seconds\n",
      "Performing whitespace count & proportion...\n",
      "Performed whitespace count & proportion in 0.8084206581115723 seconds\n",
      "Performing character count & proportion...\n",
      "Performed character count & proportion in 0.038818359375 seconds\n",
      "Performing word count...\n",
      "Performed word count in 0.19438815116882324 seconds\n",
      "Performing word length avg...\n",
      "Performed word length avg in 0.26510119438171387 seconds\n",
      "Performing word length distribution...\n",
      "Performed word length distribution in 0.3770272731781006 seconds\n",
      "Performing word count short...\n",
      "Performed word count short in 0.0142059326171875 seconds\n",
      "Performing letter case distribution...\n",
      "Performed letter case distribution in 0.5064380168914795 seconds\n",
      "Performing word case distribution...\n",
      "Performed word case distribution in 0.8166723251342773 seconds\n",
      "Performing misspellings proportion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54f249494134f27b6661d6062a34844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed misspellings proportion in 1216.4242560863495 seconds\n",
      "Performing stop words ratio...\n",
      "Performed stop words ratio in 3.233867883682251 seconds\n",
      "Performing hapax legomena proportion of total tokens...\n",
      "Performed hapax legomena proportion of total tokens in 2.8483874797821045 seconds\n",
      "Performing hapax legomena proportion of unique tokens...\n",
      "Performed hapax legomena proportion of unique tokens in 2.943195104598999 seconds\n",
      "Performing token type ratio...\n",
      "Performed token type ratio in 3.6032187938690186 seconds\n",
      "Performing train letter 1-gram...\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46303821f1b4ce6b2450379453606ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 1-gram in 7.650003910064697 seconds\n",
      "Returned up to 50 most common letter 1-grams for feature extraction on test set.\n",
      "Performing train letter 2-gram...\n",
      "['al', 'an', 'ar', 'as', 'at', 'be', 'ca', 'co', 'de', 'ea', 'ed', 'el', 'en', 'er', 'es', 'ha', 'he', 'hi', 'in', 'is', 'it', 'le', 'li', 'll', 'ly', 'me', 'nd', 'ne', 'ng', 'no', 'nt', 'of', 'om', 'on', 'or', 'ot', 'ou', 're', 'ri', 'se', 'so', 'st', 'te', 'th', 'ti', 'to', 'us', 'ut', 've', 'yo']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0e2e2f310f4c2ca6594c9e772ca6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 2-gram in 16.159202575683594 seconds\n",
      "Returned up to 50 most common letter 2-grams for feature extraction on test set.\n",
      "Performing train letter 3-gram...\n",
      "['all', 'and', 'are', 'ate', 'ati', 'ave', 'but', 'can', 'com', 'con', 'ear', 'ent', 'ere', 'ers', 'eve', 'for', 'hat', 'hav', 'her', 'hey', 'hin', 'his', 'ike', 'ing', 'ion', 'ith', 'ive', 'lly', 'nce', 'not', 'ome', 'one', 'ore', 'oul', 'our', 'out', 'ple', 'rea', 'som', 'ter', 'tha', 'the', 'thi', 'tio', 'uld', 'use', 'ust', 'ver', 'wit', 'you']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffae4e9082d64befa552ae08734f8d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train letter 3-gram in 325.5478582382202 seconds\n",
      "Returned up to 50 most common letter 3-grams for feature extraction on test set.\n",
      "Performing train digit 1-gram...\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99473e41dcb047ed9ee0cc3e851e7827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 1-gram in 3.48246693611145 seconds\n",
      "Returned up to 50 most common digit 1-grams for feature extraction on test set.\n",
      "Performing train digit 2-gram...\n",
      "['00', '01', '02', '04', '05', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '35', '40', '45', '47', '50', '52', '60', '65', '70', '75', '78', '79', '80', '90', '93', '95', '97', '98', '99']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7a5d5338bf41518803c75c9c9bc2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 2-gram in 60.71894383430481 seconds\n",
      "Returned up to 50 most common digit 2-grams for feature extraction on test set.\n",
      "Performing train digit 3-gram...\n",
      "['000', '001', '010', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '080', '090', '100', '102', '112', '120', '125', '130', '147', '150', '160', '166', '170', '180', '190', '195', '197', '198', '199', '200', '201', '202', '210', '220', '250', '300', '320', '350', '400', '500', '600', '700', '707', '716', '800', '900', '950']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d9e20a484a4d65b71c5358019e39c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train digit 3-gram in 29.81190252304077 seconds\n",
      "Returned up to 50 most common digit 3-grams for feature extraction on test set.\n",
      "Performing train punctuation 1-gram...\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '’', '“', '”']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3857cdd1b9004ee7993726b2e8c7eb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train punctuation 1-gram in 4.676625490188599 seconds\n",
      "Returned up to 50 most common punctuation 1-grams for feature extraction on test set.\n",
      "Performing train punctuation 2-gram...\n",
      "['!!', '!\"', '!)', '!<', '\"\"', '\")', '\",', '\".', '\"?', '%.', \"'.\", '(\"', '(/', '))', '),', ').', '):', '**', '*,', '*.', ',\"', '--', '->', '.\"', '.)', '.*', '.,', '..', '.]', '.”', '/)', '//', ':(', ':)', ':/', ';)', '>!', '?!', '?\"', '?)', '??', '?]', '?”', '\\\\*', '\\\\_', '](', '__', '~~', '”,', '”.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd0ef4f399342d79c1aa1da0bbaac0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train punctuation 2-gram in 47.17766237258911 seconds\n",
      "Returned up to 50 most common punctuation 2-grams for feature extraction on test set.\n",
      "Performing train word 1-gram...\n",
      "[('a',), ('about',), ('all',), ('an',), ('and',), ('are',), ('as',), ('at',), ('be',), ('but',), ('can',), ('do',), ('dont',), ('for',), ('from',), ('have',), ('i',), ('if',), ('im',), ('in',), ('is',), ('it',), ('its',), ('just',), ('like',), ('me',), ('more',), ('my',), ('not',), ('of',), ('on',), ('one',), ('or',), ('people',), ('so',), ('some',), ('that',), ('the',), ('there',), ('they',), ('this',), ('to',), ('was',), ('we',), ('what',), ('when',), ('with',), ('would',), ('you',), ('your',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54304e296ca497ab82e8e8a1a4f2319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train word 1-gram in 123.85393977165222 seconds\n",
      "Returned up to 50 most common word1-grams for feature extraction on test set.\n",
      "Performing train POS tags 1-grams...\n",
      "[('$',), (\"''\",), (',',), ('-LRB-',), ('-RRB-',), ('.',), (':',), ('ADD',), ('AFX',), ('CC',), ('CD',), ('DT',), ('EX',), ('FW',), ('HYPH',), ('IN',), ('JJ',), ('JJR',), ('JJS',), ('LS',), ('MD',), ('NFP',), ('NN',), ('NNP',), ('NNPS',), ('NNS',), ('PDT',), ('POS',), ('PRP',), ('PRP$',), ('RB',), ('RBR',), ('RBS',), ('RP',), ('SYM',), ('TO',), ('UH',), ('VB',), ('VBD',), ('VBG',), ('VBN',), ('VBP',), ('VBZ',), ('WDT',), ('WP',), ('WP$',), ('WRB',), ('XX',), ('_SP',), ('``',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf2351adc07469ca29de843706e6c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train POS tags 1-gram in 3.667933464050293 seconds\n",
      "Returned up to 50 most common POS tags 1-grams for feature extraction on test set.\n",
      "Performing train POS tags 2-grams...\n",
      "[(',', 'CC'), (',', 'PRP'), ('.', 'PRP'), ('.', 'RB'), ('CC', 'PRP'), ('DT', 'JJ'), ('DT', 'NN'), ('DT', 'NNS'), ('IN', 'DT'), ('IN', 'JJ'), ('IN', 'NN'), ('IN', 'NNP'), ('IN', 'NNS'), ('IN', 'PRP'), ('IN', 'PRP$'), ('JJ', '.'), ('JJ', 'IN'), ('JJ', 'NN'), ('JJ', 'NNS'), ('MD', 'RB'), ('MD', 'VB'), ('NN', ','), ('NN', '.'), ('NN', 'CC'), ('NN', 'IN'), ('NN', 'NN'), ('NN', 'RB'), ('NN', 'VBZ'), ('NNP', 'NNP'), ('NNS', '.'), ('NNS', 'IN'), ('PRP', 'MD'), ('PRP', 'RB'), ('PRP', 'VBD'), ('PRP', 'VBP'), ('PRP', 'VBZ'), ('PRP$', 'NN'), ('RB', '.'), ('RB', 'IN'), ('RB', 'JJ'), ('RB', 'RB'), ('RB', 'VB'), ('TO', 'VB'), ('VB', 'DT'), ('VB', 'IN'), ('VB', 'PRP'), ('VBN', 'IN'), ('VBP', 'RB'), ('VBZ', 'DT'), ('VBZ', 'RB')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b149f82929449a9bf584dd0b9b55d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train POS tags 2-gram in 20.16659665107727 seconds\n",
      "Returned up to 50 most common POS tags 2-grams for feature extraction on test set.\n",
      "Performing letter count & proportion...\n",
      "Performed letter count & proportion in 0.7388501167297363 seconds\n",
      "Performing digit count & proportion...\n",
      "Performed digit count & proportion in 0.11131525039672852 seconds\n",
      "Performing punctuation count & proportion...\n",
      "Performed punctuation count & proportion in 0.2633216381072998 seconds\n",
      "Performing whitespace count & proportion...\n",
      "Performed whitespace count & proportion in 0.07697796821594238 seconds\n",
      "Performing character count & proportion...\n",
      "Performed character count & proportion in 0.0057752132415771484 seconds\n",
      "Performing word count...\n",
      "Performed word count in 0.01598215103149414 seconds\n",
      "Performing word length avg...\n",
      "Performed word length avg in 0.021021604537963867 seconds\n",
      "Performing word length distribution...\n",
      "Performed word length distribution in 0.03286552429199219 seconds\n",
      "Performing word count short...\n",
      "Performed word count short in 0.0015408992767333984 seconds\n",
      "Performing letter case distribution...\n",
      "Performed letter case distribution in 0.13242721557617188 seconds\n",
      "Performing word case distribution...\n",
      "Performed word case distribution in 0.09869956970214844 seconds\n",
      "Performing misspellings proportion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc72aa23c534ba18357ac56bfcf6f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed misspellings proportion in 134.4947702884674 seconds\n",
      "Performing stop words ratio...\n",
      "Performed stop words ratio in 0.41547203063964844 seconds\n",
      "Performing hapax legomena proportion of total tokens...\n",
      "Performed hapax legomena proportion of total tokens in 0.3055686950683594 seconds\n",
      "Performing hapax legomena proportion of unique tokens...\n",
      "Performed hapax legomena proportion of unique tokens in 0.3615293502807617 seconds\n",
      "Performing token type ratio...\n",
      "Performed token type ratio in 1.0016300678253174 seconds\n",
      "Performing test letter 1-gram...\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faca49cc1a5c4876bea995e4bf8fd783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 1-gram in 0.30677103996276855 seconds\n",
      "Performing test letter 2-gram...\n",
      "['al', 'an', 'ar', 'as', 'at', 'be', 'ca', 'co', 'de', 'ea', 'ed', 'el', 'en', 'er', 'es', 'ha', 'he', 'hi', 'in', 'is', 'it', 'le', 'li', 'll', 'ly', 'me', 'nd', 'ne', 'ng', 'no', 'nt', 'of', 'om', 'on', 'or', 'ot', 'ou', 're', 'ri', 'se', 'so', 'st', 'te', 'th', 'ti', 'to', 'us', 'ut', 've', 'yo']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fb6c8b4f584c0081a553eb2e436ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 2-gram in 0.20581889152526855 seconds\n",
      "Performing test letter 3-gram...\n",
      "['all', 'and', 'are', 'ate', 'ati', 'ave', 'but', 'can', 'com', 'con', 'ear', 'ent', 'ere', 'ers', 'eve', 'for', 'hat', 'hav', 'her', 'hey', 'hin', 'his', 'ike', 'ing', 'ion', 'ith', 'ive', 'lly', 'nce', 'not', 'ome', 'one', 'ore', 'oul', 'our', 'out', 'ple', 'rea', 'som', 'ter', 'tha', 'the', 'thi', 'tio', 'uld', 'use', 'ust', 'ver', 'wit', 'you']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fe8cb75f0b405cb468c3083dd813c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test letter 3-gram in 0.21407389640808105 seconds\n",
      "Performing test digit 1-gram...\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ade34dbc5744bbd83e2dfbb979b76b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 1-gram in 0.33611416816711426 seconds\n",
      "Performing test digit 2-gram...\n",
      "['00', '01', '02', '04', '05', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '35', '40', '45', '47', '50', '52', '60', '65', '70', '75', '78', '79', '80', '90', '93', '95', '97', '98', '99']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f33b75d91a46febcb63a2dd9f61c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 2-gram in 0.2661018371582031 seconds\n",
      "Performing test digit 3-gram...\n",
      "['000', '001', '010', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '080', '090', '100', '102', '112', '120', '125', '130', '147', '150', '160', '166', '170', '180', '190', '195', '197', '198', '199', '200', '201', '202', '210', '220', '250', '300', '320', '350', '400', '500', '600', '700', '707', '716', '800', '900', '950']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fbf95809d543c295b5de7435ab129b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test digit 3-gram in 0.2402050495147705 seconds\n",
      "Performing test punctuation 1-gram...\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '’', '“', '”']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5fde3a934b4e508c4ea937e386ddcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test punctuation 1-gram in 0.2797088623046875 seconds\n",
      "Performing test punctuation 2-gram...\n",
      "['!!', '!\"', '!)', '!<', '\"\"', '\")', '\",', '\".', '\"?', '%.', \"'.\", '(\"', '(/', '))', '),', ').', '):', '**', '*,', '*.', ',\"', '--', '->', '.\"', '.)', '.*', '.,', '..', '.]', '.”', '/)', '//', ':(', ':)', ':/', ';)', '>!', '?!', '?\"', '?)', '??', '?]', '?”', '\\\\*', '\\\\_', '](', '__', '~~', '”,', '”.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bda152a3c21421cafa4999ea4ab0fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test punctuation 2-gram in 0.24885940551757812 seconds\n",
      "Performing test word 1-gram...\n",
      "[('a',), ('about',), ('all',), ('an',), ('and',), ('are',), ('as',), ('at',), ('be',), ('but',), ('can',), ('do',), ('dont',), ('for',), ('from',), ('have',), ('i',), ('if',), ('im',), ('in',), ('is',), ('it',), ('its',), ('just',), ('like',), ('me',), ('more',), ('my',), ('not',), ('of',), ('on',), ('one',), ('or',), ('people',), ('so',), ('some',), ('that',), ('the',), ('there',), ('they',), ('this',), ('to',), ('was',), ('we',), ('what',), ('when',), ('with',), ('would',), ('you',), ('your',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b9dad3d03a4e9a9d0fade13f540b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test word 1-gram in 0.2599010467529297 seconds\n",
      "Performing test POS tags 1-gram...\n",
      "[('$',), (\"''\",), (',',), ('-LRB-',), ('-RRB-',), ('.',), (':',), ('ADD',), ('AFX',), ('CC',), ('CD',), ('DT',), ('EX',), ('FW',), ('HYPH',), ('IN',), ('JJ',), ('JJR',), ('JJS',), ('LS',), ('MD',), ('NFP',), ('NN',), ('NNP',), ('NNPS',), ('NNS',), ('PDT',), ('POS',), ('PRP',), ('PRP$',), ('RB',), ('RBR',), ('RBS',), ('RP',), ('SYM',), ('TO',), ('UH',), ('VB',), ('VBD',), ('VBG',), ('VBN',), ('VBP',), ('VBZ',), ('WDT',), ('WP',), ('WP$',), ('WRB',), ('XX',), ('_SP',), ('``',)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c817cf56ff47fea9f5c8c1d8c342cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test POS tags 1-gram in 0.2746999263763428 seconds\n",
      "Performing test POS tags 2-gram...\n",
      "[(',', 'CC'), (',', 'PRP'), ('.', 'PRP'), ('.', 'RB'), ('CC', 'PRP'), ('DT', 'JJ'), ('DT', 'NN'), ('DT', 'NNS'), ('IN', 'DT'), ('IN', 'JJ'), ('IN', 'NN'), ('IN', 'NNP'), ('IN', 'NNS'), ('IN', 'PRP'), ('IN', 'PRP$'), ('JJ', '.'), ('JJ', 'IN'), ('JJ', 'NN'), ('JJ', 'NNS'), ('MD', 'RB'), ('MD', 'VB'), ('NN', ','), ('NN', '.'), ('NN', 'CC'), ('NN', 'IN'), ('NN', 'NN'), ('NN', 'RB'), ('NN', 'VBZ'), ('NNP', 'NNP'), ('NNS', '.'), ('NNS', 'IN'), ('PRP', 'MD'), ('PRP', 'RB'), ('PRP', 'VBD'), ('PRP', 'VBP'), ('PRP', 'VBZ'), ('PRP$', 'NN'), ('RB', '.'), ('RB', 'IN'), ('RB', 'JJ'), ('RB', 'RB'), ('RB', 'VB'), ('TO', 'VB'), ('VB', 'DT'), ('VB', 'IN'), ('VB', 'PRP'), ('VBN', 'IN'), ('VBP', 'RB'), ('VBZ', 'DT'), ('VBZ', 'RB')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb97a6d8ad444adbff9084a8cdd9660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed test POS tags 2-gram in 0.3152785301208496 seconds\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def extract_features(feeds_aug, train_or_test):\n",
    "    \n",
    "    # Number of Comments, Median comment length\n",
    "\n",
    "    feeds_aug['num_comments'] = feeds_aug['comment_lengths'].apply(len)\n",
    "    feeds_aug['comment_length_median'] = feeds_aug['comment_lengths'].apply(statistics.median)\n",
    "    feeds_aug = feeds_aug.drop('comment_lengths', axis=1)\n",
    "\n",
    "    # Character Count, Alphabet Count & Proportion, Digit Count & Proportion, Punctuation Count & Proportion\n",
    "\n",
    "    feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'letter')\n",
    "    feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'digit')\n",
    "    feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'punctuation')\n",
    "    feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'whitespace')\n",
    "    feeds_aug = character_count_proportion_wrapper(feeds_aug, 'feed_string', 'character')\n",
    "    \n",
    "    # Word Count, Average Word Length, Word Length Distribution (Freq of words length 1-20 letters), Word Case Distribution (All lowercase / First-upper-rest-lowercase / All uppercase / Other), Character case distribution (lowercase / uppercase)\n",
    "\n",
    "    word_count_wrapper(feeds_aug, 'feed_string')\n",
    "    word_length_avg_wrapper(feeds_aug, 'feed_string')\n",
    "    word_length_distribution_wrapper(feeds_aug, 'feed_string')\n",
    "    word_short_prop_wrapper(feeds_aug, 'word_length_distribution', 'word_count')\n",
    "    letter_case_distribution_wrapper(feeds_aug, 'feed_string')\n",
    "    word_case_distribution_wrapper(feeds_aug, 'feed_string')\n",
    "\n",
    "    # Misspellings Prop\n",
    "\n",
    "    misspelled_prop_wrapper(feeds_aug, 'feed_string', 'misspelled_prop')\n",
    "    \n",
    "    #Stop Word proportion of Tokens\n",
    "    stop_words_proportion_wrapper(feeds_aug, 'feed_comment_list_spacy', 'stop_words_proportion')\n",
    "    \n",
    "    # Vocabulary Richness: Hapax Legomena Proportion of Total Tokens, Hapax Legomena Proportion of Unique Tokens, Unique Tokens over Total Tokens\n",
    "    # # https://eprints.qut.edu.au/8019/1/8019.pdf\n",
    "    \n",
    "    hapax_legomena_proportion_wrapper(feeds_aug, 'feed_comment_list_spacy', 'hapax_legomena_prop_tot_tokens', 'total') # Note: ignores stop words\n",
    "    hapax_legomena_proportion_wrapper(feeds_aug, 'feed_comment_list_spacy', 'hapax_legomena_prop_unique_tokens', 'unique') # Note: ignores stop words\n",
    "    token_type_ratio_wrapper(feeds_aug, 'feed_comment_list_spacy', 'token_type_ratio') # Note: ignores stop words\n",
    "\n",
    "    if train_or_test == \"train\":\n",
    "        \n",
    "        global letter_1gram_collection_fromtrain\n",
    "        global letter_2gram_collection_fromtrain\n",
    "        global letter_3gram_collection_fromtrain\n",
    "        global letter_4gram_collection_fromtrain\n",
    "        global digit_1gram_collection_fromtrain\n",
    "        global digit_2gram_collection_fromtrain\n",
    "        global digit_3gram_collection_fromtrain\n",
    "        global punctuation_1gram_collection_fromtrain\n",
    "        global punctuation_2gram_collection_fromtrain\n",
    "        global punctuation_3gram_collection_fromtrain\n",
    "        global word_1gram_collection_fromtrain\n",
    "        global word_2gram_collection_fromtrain\n",
    "        global POS_tags_1gram_collection_fromtrain\n",
    "        global POS_tags_2gram_collection_fromtrain\n",
    "        global POS_tags_3gram_collection_fromtrain\n",
    "        \n",
    "        # Letter, Digit, and Punctuation n-grams\n",
    "        letter_1gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_1gram', 1, 'letter')\n",
    "        letter_2gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_2gram', 2, 'letter')\n",
    "        letter_3gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_3gram', 3, 'letter')\n",
    "        #letter_4gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_4gram', 4, 'letter')\n",
    "\n",
    "        digit_1gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_1gram', 1, 'digit')\n",
    "        digit_2gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_2gram', 2, 'digit')\n",
    "        digit_3gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_3gram', 3, 'digit')\n",
    "\n",
    "        punctuation_1gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_1gram', 1, 'punctuation')\n",
    "        punctuation_2gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_2gram', 2, 'punctuation')\n",
    "        #punctuation_3gram_collection_fromtrain = character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_3gram', 3, 'punctuation')\n",
    "\n",
    "        # Word ngrams\n",
    "        word_1gram_collection_fromtrain = word_ngrams_wrapper(feeds_aug, 'feed_comment_list_nopunc_lower', 'word_1gram', 1)\n",
    "        #word_2gram_collection_fromtrain = word_ngrams_wrapper(feeds_aug, 'feed_comment_list_nopunc_lower', 'word_1gram', 2)\n",
    "\n",
    "        # POS n-grams\n",
    "        POS_tags_1gram_collection_fromtrain = POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_1gram', 1)\n",
    "        POS_tags_2gram_collection_fromtrain = POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_2gram', 2)\n",
    "        #POS_tags_3gram_collection_fromtrain = POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_3gram', 3)\n",
    "\n",
    "    elif train_or_test == \"test\":\n",
    "        \n",
    "        # Letter, Digit, and Punctuation n-grams\n",
    "        character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_1gram', 1, 'letter', letter_1gram_collection_fromtrain)\n",
    "        character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_2gram', 2, 'letter', letter_2gram_collection_fromtrain)\n",
    "        character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_3gram', 3, 'letter', letter_3gram_collection_fromtrain)\n",
    "        #character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'letter_4gram', 4, 'letter', letter_4gram_collection_fromtrain)\n",
    "\n",
    "        character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_1gram', 1, 'digit', digit_1gram_collection_fromtrain)\n",
    "        character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_2gram', 2, 'digit', digit_2gram_collection_fromtrain)\n",
    "        character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'digit_3gram', 3, 'digit', digit_3gram_collection_fromtrain)\n",
    "\n",
    "        character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_1gram', 1, 'punctuation', punctuation_1gram_collection_fromtrain)\n",
    "        character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_2gram', 2, 'punctuation', punctuation_2gram_collection_fromtrain)\n",
    "        #character_ngrams_wrapper(feeds_aug, 'feed_tokens_space', 'punctuation_3gram', 3, 'punctuation', punctuation_3gram_collection_fromtrain)\n",
    "\n",
    "        # Word ngrams\n",
    "        word_ngrams_wrapper(feeds_aug, 'feed_comment_list_nopunc_lower', 'word_1gram', 1, word_1gram_collection_fromtrain)\n",
    "        #word_ngrams_wrapper(feeds_aug, 'feed_comment_list_nopunc_lower', 'word_1gram', 2, word_2gram_collection_fromtrain)\n",
    "\n",
    "        # POS n-grams\n",
    "        POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_1gram', 1, POS_tags_1gram_collection_fromtrain)\n",
    "        POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_2gram', 2, POS_tags_2gram_collection_fromtrain)\n",
    "        #POS_tags_ngram_wrapper(feeds_aug, 'feed_comment_list_spacy', 'POS_tag_3gram', 3, POS_tags_3gram_collection_fromtrain)\n",
    "\n",
    "    return feeds_aug\n",
    "\n",
    "X_train = extract_features(X_train, \"train\")\n",
    "X_test = extract_features(X_test, \"test\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def expand_feature_set(dataframe):\n",
    "    dataframe = dataframe[['proficiency', 'comment_length_median', 'letter_prop', 'digit_prop', 'punctuation_prop', 'whitespace_prop', 'word_length_avg', 'word_length_distribution', 'word_short_prop', 'letter_case_distribution', 'word_case_distribution', 'misspelled_prop', 'stop_words_proportion', 'hapax_legomena_prop_tot_tokens', 'hapax_legomena_prop_unique_tokens', 'token_type_ratio', 'letter_1gram', 'letter_2gram', 'letter_3gram', 'digit_1gram', 'digit_2gram', 'digit_3gram', 'punctuation_1gram', 'punctuation_2gram', 'word_1gram', 'POS_tag_1gram', 'POS_tag_2gram']]\n",
    "    for col in dataframe.columns:\n",
    "        if type(dataframe[col].iloc[0]) == list:\n",
    "            newcols = [col + \"_\" + str(i) for i in range(1,len(dataframe[col].iloc[0]) + 1)]\n",
    "            dataframe[newcols] = pd.DataFrame(dataframe[col].tolist(), index= dataframe.index)\n",
    "            dataframe = dataframe.drop([col], axis = 1)\n",
    "        elif type(dataframe[col].iloc[0]) == str and col != 'proficiency':\n",
    "            dataframe[col] = pd.to_numeric(dataframe[col], downcast=\"float\")\n",
    "            \n",
    "    return dataframe\n",
    "\n",
    "X_train = expand_feature_set(X_train)\n",
    "X_test = expand_feature_set(X_test)\n",
    "\n",
    "X_train.to_pickle(\"X_train_w_features.pkl\")\n",
    "X_test.to_pickle(\"X_test_w_features.pkl\")\n",
    "y_train.to_pickle(\"y_train.pkl\")\n",
    "y_test.to_pickle(\"y_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4739e0-a2d0-4f71-b323-b4dc398acc50",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad4b355-64b4-40e7-a85d-91b546e9840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.baeldung.com/cs/svm-multiclass-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0f6f939-5ec4-42fc-8ba5-08450dd5ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVM with linear Kernel:\n",
      " \n",
      "- Accuracy\n",
      "-- Overall:  60.00\n",
      "-- Native: 66.67\n",
      "-- Non-Native: 51.85\n",
      " \n",
      "- F1 Score\n",
      "-- Overall: 59.03\n",
      "-- Native: 70.25\n",
      "-- Non-Native: 56.49\n",
      " \n",
      " \n",
      "Results are for 15 native + 15 non-native english authors. Model trained on 1,080 feeds and tested on 120 feeds.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Results for SVM with poly Kernel:\n",
      " \n",
      "- Accuracy\n",
      "-- Overall:  2.50\n",
      "-- Native: 4.55\n",
      "-- Non-Native: 0.00\n",
      " \n",
      "- F1 Score\n",
      "-- Overall: 1.45\n",
      "-- Native: 2.84\n",
      "-- Non-Native: 0.00\n",
      " \n",
      " \n",
      "Results are for 15 native + 15 non-native english authors. Model trained on 1,080 feeds and tested on 120 feeds.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Results for SVM with rbf Kernel:\n",
      " \n",
      "- Accuracy\n",
      "-- Overall:  1.67\n",
      "-- Native: 0.00\n",
      "-- Non-Native: 3.70\n",
      " \n",
      "- F1 Score\n",
      "-- Overall: 0.35\n",
      "-- Native: 0.00\n",
      "-- Non-Native: 1.30\n",
      " \n",
      " \n",
      "Results are for 15 native + 15 non-native english authors. Model trained on 1,080 feeds and tested on 120 feeds.\n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Import train and test data w features. Split out proficiency.\n",
    "X_train = pd.read_pickle('X_train_w_features.pkl')\n",
    "train_proficiency = X_train['proficiency']\n",
    "X_train = X_train.drop(['proficiency'], axis = 1)\n",
    "\n",
    "X_test = pd.read_pickle('X_test_w_features.pkl')\n",
    "test_proficiency = X_test['proficiency']\n",
    "X_test = X_test.drop(['proficiency'], axis = 1)\n",
    "\n",
    "# Train SVM models\n",
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(X_train, y_train['author_id'])\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1).fit(X_train, y_train['author_id'])\n",
    "linear = svm.SVC(kernel='linear', degree=3, C=1).fit(X_train, y_train['author_id'])\n",
    "\n",
    "# Split test into native and nonnative feeds\n",
    "X_test_all_proficiencies = X_test.merge(test_proficiency, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "X_test_native = X_test_all_proficiencies.loc[X_test_all_proficiencies['proficiency'] == \"N\"]\n",
    "y_test_native = y_test['author_id'].loc[X_test_all_proficiencies['proficiency'] == \"N\"]\n",
    "X_test_native = X_test_native.drop(['proficiency'], axis = 1)\n",
    "\n",
    "X_test_nonnat = X_test_all_proficiencies.loc[X_test_all_proficiencies['proficiency'] == \"L\"]\n",
    "y_test_nonnat = y_test['author_id'].loc[X_test_all_proficiencies['proficiency'] == \"L\"]\n",
    "X_test_nonnat = X_test_nonnat.drop(['proficiency'], axis = 1)\n",
    "\n",
    "# Predict native and non-native mode performance\n",
    "def predict_and_evaluate_svm(model):\n",
    "    # Predict native and non-native\n",
    "    pred_all = model.predict(X_test)\n",
    "    pred_native = model.predict(X_test_native)\n",
    "    pred_nonnat = model.predict(X_test_nonnat)\n",
    "\n",
    "    accuracy_all = accuracy_score(y_test['author_id'], pred_all)\n",
    "    accuracy_native = accuracy_score(y_test_native, pred_native)\n",
    "    accuracy_nonnat = accuracy_score(y_test_nonnat, pred_nonnat)\n",
    "    f1_all = f1_score(y_test['author_id'], pred_all, average='weighted')\n",
    "    f1_native = f1_score(y_test_native, pred_native, average='weighted')\n",
    "    f1_nonnat = f1_score(y_test_nonnat, pred_nonnat, average='weighted')\n",
    "    \n",
    "    print('Results for SVM with ' + model.kernel + ' Kernel:')\n",
    "    print(' ')\n",
    "    print('- Accuracy')\n",
    "    print('-- Overall: ', \"%.2f\" % (accuracy_all*100))\n",
    "    print('-- Native:', \"%.2f\" % (accuracy_native*100))\n",
    "    print('-- Non-Native:', \"%.2f\" % (accuracy_nonnat*100))\n",
    "    print(' ')\n",
    "    print('- F1 Score')\n",
    "    print('-- Overall:', \"%.2f\" % (f1_all*100))\n",
    "    print('-- Native:', \"%.2f\" % (f1_native*100))\n",
    "    print('-- Non-Native:', \"%.2f\" % (f1_nonnat*100))\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print('Results are for 15 native + 15 non-native english authors. Model trained on 1,080 feeds and tested on 120 feeds.') \n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    \n",
    "predict_and_evaluate_svm(linear)\n",
    "predict_and_evaluate_svm(poly)\n",
    "predict_and_evaluate_svm(rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783c560-df16-4fdf-a020-f89e0f405c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
