{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import langdetect\n",
    "import random\n",
    "from preprocess import *\n",
    "\n",
    "\n",
    "random.seed(22)\n",
    "np.random.seed(22)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First step with the data, creation of the dataframe ready to be passed on langdetect module\n",
    "\n",
    "Langdetect module take times to run over 1M comments and therefore we first sort out all the useless data (authors with less than 10000 words, authors with a flair not detected by the parser, author spamming the same comments and all the comments that are just a link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 2144 users.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>N</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jlba64</td>\n",
       "      <td>['fr']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>['de']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ko']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['en']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Noktilucent</td>\n",
       "      <td>['en']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['de', 'it']</td>\n",
       "      <td>['es']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>makingthematrix</td>\n",
       "      <td>['pl']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jammal20</td>\n",
       "      <td>['ar']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['tr', 'es']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['he']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kaeya_lilies</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pidgeon-eater-69</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Frenes</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>himit</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ja']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ii_akinae_ii</td>\n",
       "      <td>['en']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['zh']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author       N  A1            A2      B1      B2      C1      C2\n",
       "0            jlba64  ['fr']  []            []      []      []      []      []\n",
       "1          alexsteb  ['de']  []            []  ['ko']      []  ['en']      []\n",
       "2       Noktilucent  ['en']  []  ['de', 'it']  ['es']      []      []      []\n",
       "3   makingthematrix  ['pl']  []            []      []      []      []      []\n",
       "4          jammal20  ['ar']  []  ['tr', 'es']      []  ['he']      []      []\n",
       "5      kaeya_lilies      []  []            []      []      []      []      []\n",
       "6  pidgeon-eater-69      []  []            []      []      []      []      []\n",
       "7            Frenes      []  []            []      []      []      []      []\n",
       "8             himit      []  []            []      []      []      []  ['ja']\n",
       "9      ii_akinae_ii  ['en']  []            []  ['zh']      []      []      []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset containing all the Reddit users \n",
    "levels_df = pd.read_csv('../../Data/Raw/user_levels.csv')\n",
    "\n",
    "# Reduce the dataframe with usefull information\n",
    "levels_df = levels_df[['author', 'N', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2']].reset_index(drop = True)\n",
    "\n",
    "print('The dataset contains', len(levels_df), 'users.')\n",
    "levels_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put into a list of all the authors within the dataframe\n",
    "authors = levels_df.author.tolist()\n",
    "\n",
    "path_to_json = '../../Data/Raw/user_comments/'\n",
    "frames = []\n",
    "\n",
    "# Import all the comments of the authors within the dataframe and put them into the original dataframe\n",
    "for author in authors:\n",
    "    fpath = path_to_json + author + '.json'\n",
    "\n",
    "    # Avoid empty file and suspended/deleted accounts\n",
    "    if is_non_zero_file(fpath):\n",
    "        df = pd.read_json(fpath)\n",
    "        if (df.iloc[0][0] != 'suspended') and (df.iloc[0][0] != 'deleted'):\n",
    "            frames.append(df)\n",
    "        \n",
    "comments_df = pd.concat(frames)\n",
    "comments_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>number_of_words</th>\n",
       "      <th>body_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>Check out TalkToMeInKorean. They at least HAD ...</td>\n",
       "      <td>1666878614</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>It's a toss up between Japanese, Korean and Ma...</td>\n",
       "      <td>1666874877</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>I've seen something like those also in Göreme,...</td>\n",
       "      <td>1666793085</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>Wrong tones are like wrong vowels. It's still ...</td>\n",
       "      <td>1666767494</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>\\*als du es dir vorgestellt hast.</td>\n",
       "      <td>1666767139</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                               body  created_utc  \\\n",
       "0  alexsteb  Check out TalkToMeInKorean. They at least HAD ...   1666878614   \n",
       "1  alexsteb  It's a toss up between Japanese, Korean and Ma...   1666874877   \n",
       "2  alexsteb  I've seen something like those also in Göreme,...   1666793085   \n",
       "3  alexsteb  Wrong tones are like wrong vowels. It's still ...   1666767494   \n",
       "4  alexsteb                  \\*als du es dir vorgestellt hast.   1666767139   \n",
       "\n",
       "   number_of_words  body_lang  \n",
       "0               16        NaN  \n",
       "1               49        NaN  \n",
       "2               14        NaN  \n",
       "3               21        NaN  \n",
       "4                6        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all duplicates because some commenters are spamming the same thing -->  biases for ML\n",
    "comments_df.drop_duplicates(subset = ['author', 'body'], keep = 'first', inplace = True)\n",
    "\n",
    "# Reduce the dataframe and count the number of words per author\n",
    "comments_df = comments_df[['author', 'body', 'created_utc']]\n",
    "comments_df['number_of_words'] = comments_df['body'].str.split().str.len()\n",
    "\n",
    "# Keep only authors we more than 10'000 words written\n",
    "comments_df_sum = comments_df[['author', 'number_of_words']].groupby('author').agg('sum')\n",
    "kept_authors = comments_df_sum[comments_df_sum.number_of_words >= 10000].index.tolist()\n",
    "comments_df = comments_df[comments_df['author'].isin(kept_authors)]\n",
    "\n",
    "# Remove all the comments with only a link and nothing else\n",
    "link = ((comments_df.number_of_words <= 1) & (comments_df.body.str[:4] == 'http'))\n",
    "comments_df = comments_df[~link]\n",
    "\n",
    "# Reset the index\n",
    "comments_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Save the dataframe\n",
    "comments_df.to_pickle('../../Data/Preprocessing/processed_comments_all_flairs.pkl')\n",
    "\n",
    "comments_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the processed df \n",
    "comments_lang = pd.read_pickle('../../Data/Preprocessing/processed_comments_all_flairs.pkl')\n",
    "\n",
    "# put all the comments into a list of string\n",
    "comments = comments_lang.body.to_list()\n",
    "\n",
    "# For all comments, put in the column body_lan the language the the highest probability. \n",
    "# If no language is detected, put 'U' as undefined\n",
    "\n",
    "language = []\n",
    "for comment in tqdm(comments):\n",
    "    try: \n",
    "        langs = langdetect.detect_langs(comment)\n",
    "        language.append(langs[0].lang)\n",
    "    except:\n",
    "        language.append('U')\n",
    "\n",
    "# Add the column with the language for each comments\n",
    "comments_lang['body_lang'] = language\n",
    "\n",
    "# Save the new dataframe \n",
    "comments_lang.to_pickle('../../Data/Preprocessing/langdetect_classification_all.pkl')\n",
    "\n",
    "comments_lang.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Keep only the authors with more than 10 000 words written in english and generate feeds\n",
    "\n",
    "> * It appeared that after language detection, English was the only language with enougth comments to work on. All the others language will > then be dropped. \n",
    "> * It also appeared than there is not enough of authors writing in english per proficiency, we will thus use 'Native' and 'Learners' for \n",
    "> our research question.\n",
    "> * Feeds are documents containing ~ 500 words for each authors in which the features will be developped. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>body_lang</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>number_of_words</th>\n",
       "      <th>N</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>Check out TalkToMeInKorean. They at least HAD ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666879e+09</td>\n",
       "      <td>16.0</td>\n",
       "      <td>['de']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ko']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['en']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>It's a toss up between Japanese, Korean and Ma...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666875e+09</td>\n",
       "      <td>49.0</td>\n",
       "      <td>['de']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ko']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['en']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>I've seen something like those also in Göreme,...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666793e+09</td>\n",
       "      <td>14.0</td>\n",
       "      <td>['de']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ko']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['en']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>Wrong tones are like wrong vowels. It's still ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666767e+09</td>\n",
       "      <td>21.0</td>\n",
       "      <td>['de']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ko']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['en']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>Don't have to time to try it now, but I love y...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666702e+09</td>\n",
       "      <td>19.0</td>\n",
       "      <td>['de']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ko']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['en']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                               body body_lang  \\\n",
       "0  alexsteb  Check out TalkToMeInKorean. They at least HAD ...        en   \n",
       "1  alexsteb  It's a toss up between Japanese, Korean and Ma...        en   \n",
       "2  alexsteb  I've seen something like those also in Göreme,...        en   \n",
       "3  alexsteb  Wrong tones are like wrong vowels. It's still ...        en   \n",
       "4  alexsteb  Don't have to time to try it now, but I love y...        en   \n",
       "\n",
       "    created_utc  number_of_words       N  A1  A2      B1  B2      C1  C2  \n",
       "0  1.666879e+09             16.0  ['de']  []  []  ['ko']  []  ['en']  []  \n",
       "1  1.666875e+09             49.0  ['de']  []  []  ['ko']  []  ['en']  []  \n",
       "2  1.666793e+09             14.0  ['de']  []  []  ['ko']  []  ['en']  []  \n",
       "3  1.666767e+09             21.0  ['de']  []  []  ['ko']  []  ['en']  []  \n",
       "4  1.666702e+09             19.0  ['de']  []  []  ['ko']  []  ['en']  []  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the users levels dataset which has been cleaned up\n",
    "user_df = pd.read_csv('../../Data/Raw/user_levels.csv')\n",
    "\n",
    "#Import the dataset containing the comments AFTER language detection\n",
    "classified_comments_df = pd.read_pickle('Data/Preprocessing/english_comments_2') # This file contains the results of language detection with langdetect completed by hands\n",
    "\n",
    "# Merge the two datasets\n",
    "comments_user_df = pd.merge(classified_comments_df, user_df, left_on= 'author', right_on= 'author').drop(['flair', 'Unnamed: 0'], axis = 1)\n",
    "\n",
    "# Drop all the comments that are not detected to be in English\n",
    "comments_user_df = comments_user_df[comments_user_df.body_lang == 'en']\n",
    "\n",
    "comments_user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>body_lang</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>number_of_words</th>\n",
       "      <th>proficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>Check out TalkToMeInKorean. They at least HAD ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666879e+09</td>\n",
       "      <td>16.0</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>It's a toss up between Japanese, Korean and Ma...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666875e+09</td>\n",
       "      <td>49.0</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>I've seen something like those also in Göreme,...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666793e+09</td>\n",
       "      <td>14.0</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>Wrong tones are like wrong vowels. It's still ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666767e+09</td>\n",
       "      <td>21.0</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alexsteb</td>\n",
       "      <td>Don't have to time to try it now, but I love y...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.666702e+09</td>\n",
       "      <td>19.0</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                               body body_lang  \\\n",
       "0  alexsteb  Check out TalkToMeInKorean. They at least HAD ...        en   \n",
       "1  alexsteb  It's a toss up between Japanese, Korean and Ma...        en   \n",
       "2  alexsteb  I've seen something like those also in Göreme,...        en   \n",
       "3  alexsteb  Wrong tones are like wrong vowels. It's still ...        en   \n",
       "4  alexsteb  Don't have to time to try it now, but I love y...        en   \n",
       "\n",
       "    created_utc  number_of_words proficiency  \n",
       "0  1.666879e+09             16.0          C1  \n",
       "1  1.666875e+09             49.0          C1  \n",
       "2  1.666793e+09             14.0          C1  \n",
       "3  1.666767e+09             21.0          C1  \n",
       "4  1.666702e+09             19.0          C1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only authors with a known proficiency in English \n",
    "proficiency = []\n",
    "levels = ['N', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "for index, row in comments_user_df.iterrows():\n",
    "    counter = 0\n",
    "    for col in levels:\n",
    "        if ('en' in row[col]) | ('En' in row[col]) | ('eN' in row[col]) | ('EN' in row[col]):\n",
    "            proficiency.append(col)\n",
    "            break\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 7:\n",
    "            proficiency.append('None')\n",
    "\n",
    "# Add the column proficiency and drop the levels columns\n",
    "comments_user_df['proficiency'] = proficiency\n",
    "comments_user_df.drop(levels, axis = 1, inplace=True)\n",
    "\n",
    "# Drop the line whithout a known proficiency in english\n",
    "comments_user_df = comments_user_df[comments_user_df.proficiency != 'None']\n",
    "\n",
    "comments_user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 640/640 [00:01<00:00, 591.33it/s]\n",
      "100%|██████████| 279/279 [00:00<00:00, 872.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new dataset contains 354 native and 135 non-native authors left for the development and evaluation stage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into native and non-native authors\n",
    "bool_native = (comments_user_df.proficiency == 'N')\n",
    "natives_df = comments_user_df[bool_native]\n",
    "learners_df = comments_user_df[~bool_native]\n",
    "\n",
    "# Generate feeds for native and non native speakers separately \n",
    "native_feeds = generate_feeds(natives_df, nb_feeds = 20, nb_words_per_feed = 500, exact = False, seed = 0)\n",
    "non_native_feeds = generate_feeds(learners_df, nb_feeds = 20, nb_words_per_feed = 500, exact = False, seed = 0)\n",
    "\n",
    "print('The new datasets contains ',len(native_feeds), 'native and', len(non_native_feeds), 'non-native authors left for the developpment and evaluation stage.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the newly generated dataframe into pickle file\n",
    "native_feeds.to_pickle('../../Data/Preprocessing/native_english_20feeds')\n",
    "non_native_feeds.to_pickle('../../Data/Preprocessing/non_native_english_20feeds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split the cohorts for the developpment and evaluation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file just created\n",
    "native_feeds = pd.read_pickle('../../Data/Preprocessing/native_english_20feeds')\n",
    "non_native_feeds = pd.read_pickle('../../Data/Preprocessing/non_native_english_20feeds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 dataframes for the developpment stage \n",
    "# with 30 randomly picked natives for the first dataframe and 30 randomly picked non-natives for the other one.\n",
    "tunning_samples_native     = random.sample(range(len(native_feeds)), 30)\n",
    "tunning_samples_non_native = random.sample(range(len(non_native_feeds)), 30)\n",
    "\n",
    "native_authors_tunning = native_feeds.iloc[tunning_samples_native]\n",
    "non_native_authors_tunning = non_native_feeds.iloc[tunning_samples_non_native]\n",
    "\n",
    "# Save these new dataframes into a parquet file (in order to be tuned on SCITAS)\n",
    "native_authors_tunning.to_parquet('../../Data/Tuning/30native_english')\n",
    "non_native_authors_tunning.to_parquet('../../Data/Tuning/30non_native_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 dataframes for the evluation stage\n",
    "#  with 100 randomly picked natives for the first dataframe and 100 randomly picked non-natives for the other one.\n",
    "\n",
    "# Drop the authors who have already been taken in the train set\n",
    "test_native_feeds     = native_feeds.drop(index = native_authors_tunning.index)\n",
    "test_non_native_feeds = non_native_feeds.drop(index = non_native_authors_tunning.index)\n",
    "\n",
    "# Chose randomly 90 native authors and 90 non-native authors among the 100 remaining\n",
    "test_samples_native     = random.sample(range(len(test_native_feeds)), 90)\n",
    "test_samples_non_native = random.sample(range(len(test_non_native_feeds)), 90)\n",
    "\n",
    "for i in range(3):\n",
    "    native_authors_testing = test_native_feeds.iloc[test_samples_native[30*i:30*(i+1)]]\n",
    "    non_native_authors_testing = test_non_native_feeds.iloc[test_samples_non_native[30*i:30*(i+1)]]\n",
    "    native_authors_testing.to_pickle(f'../../Data/Test/30native_english{i+1}.pkl')\n",
    "    non_native_authors_testing.to_pickle(f'../../Data/Test/30non_native_english{i+1}.pkl') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
