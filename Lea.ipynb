{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal : find french and english comments in a given set of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages used afterwards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Language_Detection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe containing the comments to classify\n",
    "\n",
    "#list = [['et oui mais bon je voudrais pas dire dass ich nicht hier bin, weil du','Hello there, what are you doing?','Bonjour, eh oui c\\'est pas poli','Check out TalkToMeInKorean. They at least HAD something like that, don\\'t know their current product offering.','als du es dir vorgestellt hast.','Don\\'t have to time to try it now, but I love your website (and app) design. Nice popping colors.'],[-1,-1,-1,-1,-1,-1]]\n",
    "#initial_df = pd.DataFrame(list, index=['body', 'body_lang']).T\n",
    "initial_df = pd.read_pickle('Data/processed_comments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language classification using the package langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform langdetect classification (=python package)\n",
    "langdetect_classification_df = langdetect_dataframe(initial_df,seed=4)\n",
    "langdetect_classification_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to a pickle file\n",
    "langdetect_classification_df.to_pickle('Data/Classified/langdetect_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langdetect_classification_df[['body_lang']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove undefined featuring urls and then perform a new language detection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file saved after having run the previous part \n",
    "df = pd.read_pickle('Data/Classified/langdetect_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all hyperlinks \n",
    "links = (df['body'].str.contains('http')) & (df['body_lang']=='U')\n",
    "df1 = df[~links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the package detectlang again, only on undefined comments, with another seed\n",
    "df2 = langdetect_dataframe(df1[df1['body_lang']=='U'],seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Save it to a pickle file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df1\u001b[39m.\u001b[39mto_pickle(\u001b[39m'\u001b[39m\u001b[39mData/Classified/langdetect_classification_1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df2\u001b[39m.\u001b[39mto_pickle(\u001b[39m'\u001b[39m\u001b[39mData/Classified/langdetect_classification_2\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "# Save it to a pickle file\n",
    "df1.to_pickle('Data/Classified/langdetect_classification_1')\n",
    "df2.to_pickle('Data/Classified/langdetect_classification_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file we saved before\n",
    "df2= pd.read_pickle('Data/Classified/langdetect_classification_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the newly classified comments in the previous dataframe\n",
    "df3 = df2.combine_first(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body_lang\n",
       "en           633544\n",
       "N             70993\n",
       "fr            11253\n",
       "U              8035\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[['body_lang']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_fr = df3[df3['body_lang']=='fr']\n",
    "df4_en = df3[df3['body_lang']=='en']\n",
    "df4_other = df3[df3['body_lang']=='N']\n",
    "df4_undef = df3[df3['body_lang']=='U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were  980  authors with >5000 words\n",
      "And after this classification, \n",
      "14   for french\n",
      "954  for english\n"
     ]
    }
   ],
   "source": [
    "N = df1[['author']].nunique().item()\n",
    "df_sum_fr = df4_fr.groupby('author').number_of_words.agg('sum')\n",
    "df_sum_en = df4_en.groupby('author').number_of_words.agg('sum')\n",
    "print(\"There were \",N,\" authors with >5000 words\")\n",
    "print(\"And after this classification, \")\n",
    "print((df_sum_fr >= 5000).sum(), \"  for french\")\n",
    "print((df_sum_en >= 5000).sum(), \" for english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all frames to pickle files\n",
    "df3.to_pickle('Data/Classified/langdetect_classification_3')\n",
    "df4_fr.to_pickle('Data/Classified/french_comments')\n",
    "df4_en.to_pickle('Data/Classified/english_comments')\n",
    "df4_other.to_pickle('Data/Classified/other_language_comments')\n",
    "df4_undef.to_pickle('Data/Classified/undefined_comments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human classification ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify by hand the undefined comments (where the package langdetect couldn't provide a satisfying answer)\n",
    "human_classification_df = human_class_df(langdetect_classification_df)\n",
    "human_classification_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to a csv file\n",
    "human_classification_df.to_csv('Data/Classified/human_classification.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
