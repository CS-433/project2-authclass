{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal : find french and english comments in a given set of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages used afterwards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Language_Detection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe containing the comments to classify\n",
    "\n",
    "#list = [['et oui mais bon je voudrais pas dire dass ich nicht hier bin, weil du','Hello there, what are you doing?','Bonjour, eh oui c\\'est pas poli','Check out TalkToMeInKorean. They at least HAD something like that, don\\'t know their current product offering.','als du es dir vorgestellt hast.','Don\\'t have to time to try it now, but I love your website (and app) design. Nice popping colors.'],[-1,-1,-1,-1,-1,-1]]\n",
    "#initial_df = pd.DataFrame(list, index=['body', 'body_lang']).T\n",
    "initial_df = pd.read_pickle('Data/processed_comments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language classification using the package langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform langdetect classification (=python package)\n",
    "langdetect_classification_df = langdetect_dataframe(initial_df,seed=4)\n",
    "langdetect_classification_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to a pickle file\n",
    "langdetect_classification_df.to_pickle('Data/Classified/langdetect_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langdetect_classification_df[['body_lang']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove undefined featuring urls and then perform a new language detection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file saved after having run the previous part \n",
    "df = pd.read_pickle('Data/Classified/langdetect_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all hyperlinks \n",
    "links = (df['body'].str.contains('http')) & (df['body_lang']=='U')\n",
    "df1 = df[~links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the package detectlang again, only on undefined comments, with another seed\n",
    "df2 = langdetect_dataframe(df1[df1['body_lang']=='U'],seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to a pickle file\n",
    "df1.to_pickle('Data/Classified/langdetect_classification_1')\n",
    "df2.to_pickle('Data/Classified/langdetect_classification_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file we saved before\n",
    "df2= pd.read_pickle('Data/Classified/langdetect_classification_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the newly classified comments in the previous dataframe\n",
    "df3 = df2.combine_first(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[['body_lang']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_fr = df3[df3['body_lang']=='fr']\n",
    "df4_en = df3[df3['body_lang']=='en']\n",
    "df4_other = df3[df3['body_lang']=='N']\n",
    "df4_undef = df3[df3['body_lang']=='U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df1[['author']].nunique().item()\n",
    "df_sum_fr = df4_fr.groupby('author').number_of_words.agg('sum')\n",
    "df_sum_en = df4_en.groupby('author').number_of_words.agg('sum')\n",
    "print(\"There were \",N,\" authors with >5000 words\")\n",
    "print(\"And after this classification, \")\n",
    "print((df_sum_fr >= 5000).sum(), \"  for french\")\n",
    "print((df_sum_en >= 5000).sum(), \" for english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all frames to pickle files\n",
    "df3.to_pickle('Data/Classified/langdetect_classification_3')\n",
    "df4_fr.to_pickle('Data/Classified/french_comments')\n",
    "df4_en.to_pickle('Data/Classified/english_comments')\n",
    "df4_other.to_pickle('Data/Classified/other_language_comments')\n",
    "df4_undef.to_pickle('Data/Classified/undefined_comments')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3       = pd.read_pickle('Data/Classified/langdetect_classification_3')\n",
    "df4_fr    = pd.read_pickle('Data/Classified/french_comments')\n",
    "df4_en    = pd.read_pickle('Data/Classified/english_comments')\n",
    "df4_other = pd.read_pickle('Data/Classified/other_language_comments')\n",
    "df4_undef = pd.read_pickle('Data/Classified/undefined_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human classification ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be careful with the order of the cells!**\n",
    "````{verbatim}\n",
    "---> Begining : load a file containing undefined comments\n",
    "^  v\n",
    "^  v\n",
    "^  v      ....classify.....\n",
    "^  v\n",
    "^  v\n",
    "<--- End : overwrite new language attribution (over the previous 'U') for each comment you classified\n",
    " \n",
    "So, be sure you run the last cells to not lose what you've just classified ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA TO CLASSIFY \n",
    "# df_non_splitted = the whole set of comments (fr,en,U and N)\n",
    "# df_to_classify  = the comments whose language is still undefined \n",
    "df_non_splitted = pd.read_pickle('Data/Classified/test_1')\n",
    "\n",
    "\n",
    "# This blocks will set body_lang = 'N' for any comments <= 2 words starting with /r, /u, or ![gif]\n",
    "def fixer(body):\n",
    "    if body.startswith('/u'):\n",
    "        return \"N\"\n",
    "    elif body.startswith('/r'):\n",
    "        return \"N\"\n",
    "    if body.startswith('![gif]'):\n",
    "        return \"N\"  \n",
    "    else:\n",
    "        return \n",
    "def fixer_wrapper(dataframe):\n",
    "    dataframe['body_lang_2'] = dataframe['body'].apply(fixer) \n",
    "    \n",
    "fixer_wrapper(df_non_splitted)\n",
    "df_non_splitted['body_lang'] = np.where((df_non_splitted['body_lang_2'] == \"N\") & (df_non_splitted['number_of_words'] < 2), df_non_splitted['body_lang_2'], df_non_splitted['body_lang'])\n",
    "\n",
    "df_to_classify  = df_non_splitted[df_non_splitted['body_lang']=='U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify by hand until you're bored \n",
    "# Type : 0 for non english and non french,\n",
    "#        1 for english\n",
    "#        2 for french\n",
    "# Type   e to exit the function \n",
    "df_partially_classified = human_class_df(df_to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>number_of_words</th>\n",
       "      <th>body_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author, body, created_utc, number_of_words, body_lang]\n",
       "Index: []"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if wou wanrt to see what you classified (change the argument to see more lines)\n",
    "df_partially_classified.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put what has been classified into the non splitted dataframe\n",
    "df_non_splitted_new = df_partially_classified.combine_first(df_non_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new data to pickle files\n",
    "df_non_splitted_new.to_pickle('Data/Classified/test_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
